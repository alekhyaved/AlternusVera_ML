{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Polynomial_Equation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-4zILkyG_QA"
      },
      "source": [
        "#Polynomial Equation : Predicting the fakeness score\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8zJg6fCGaQr"
      },
      "source": [
        "##Team Seekers\n",
        "\n",
        "\n",
        "*   Anvitha Karanam : BERT Multiclass classification\n",
        "*   Jahnavi Rangu : Stance Detection\n",
        "*   Leela Alekhya Vedula : Spam Detection\n",
        "*   Manisha Yacham : Clickbait\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdgA2MAYHQmb"
      },
      "source": [
        "Google drive authentication lo load individual factor files (.py classes):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp0lDaJ5nVxe"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK7-JN4Cn0gK"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OwnWebUIWpO"
      },
      "source": [
        "##**BERT Multiclass classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFVqPCv3FsGP"
      },
      "source": [
        "**Installing transformers for BERT Multiclass classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAKOhqVwiq_E",
        "outputId": "c7b6ac2e-0944-4fb9-b151-9599986045b1"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\r\u001b[K     |▎                               | 10kB 24.4MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 32.0MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 22.0MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 20.6MB/s eta 0:00:01\r\u001b[K     |█▎                              | 51kB 21.2MB/s eta 0:00:01\r\u001b[K     |█▌                              | 61kB 23.3MB/s eta 0:00:01\r\u001b[K     |█▊                              | 71kB 24.9MB/s eta 0:00:01\r\u001b[K     |██                              | 81kB 26.7MB/s eta 0:00:01\r\u001b[K     |██▎                             | 92kB 21.9MB/s eta 0:00:01\r\u001b[K     |██▌                             | 102kB 23.3MB/s eta 0:00:01\r\u001b[K     |██▊                             | 112kB 23.3MB/s eta 0:00:01\r\u001b[K     |███                             | 122kB 23.3MB/s eta 0:00:01\r\u001b[K     |███▎                            | 133kB 23.3MB/s eta 0:00:01\r\u001b[K     |███▌                            | 143kB 23.3MB/s eta 0:00:01\r\u001b[K     |███▉                            | 153kB 23.3MB/s eta 0:00:01\r\u001b[K     |████                            | 163kB 23.3MB/s eta 0:00:01\r\u001b[K     |████▎                           | 174kB 23.3MB/s eta 0:00:01\r\u001b[K     |████▌                           | 184kB 23.3MB/s eta 0:00:01\r\u001b[K     |████▉                           | 194kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 204kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 215kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 225kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 235kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 245kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 256kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 266kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 276kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 286kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 296kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 307kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 317kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 327kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 337kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 348kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 358kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 368kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 378kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 389kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 399kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 409kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 419kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 430kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 440kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 450kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 460kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 471kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 481kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 491kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 501kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 512kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 522kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 532kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 542kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 552kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 563kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 573kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 583kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 593kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 604kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 614kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 624kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 634kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 645kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 655kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 665kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 675kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 686kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 696kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 706kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 716kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 727kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 737kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 747kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 757kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 768kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 778kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 788kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 798kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 808kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 819kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 829kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 839kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 849kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 860kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 870kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 880kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 890kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 901kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 911kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 921kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 931kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 942kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 952kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 962kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 972kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 983kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 993kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.0MB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.0MB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.0MB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.0MB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.0MB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1MB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.1MB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.1MB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.1MB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1MB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.1MB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.1MB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.1MB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.1MB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.2MB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.2MB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.2MB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.2MB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.2MB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.2MB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.2MB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.2MB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2MB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.3MB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.3MB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.3MB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.3MB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 23.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 54.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 54.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 53.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=05f1a24169f472c86176432b35a6a865d4f870d876ccec5e61c66a73cf1df726\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueUTbGlxpHXp",
        "outputId": "12512ee2-9317-4bd5-bc2f-b78db1af7d00"
      },
      "source": [
        "!wget -O bertmodel https://www.dropbox.com/sh/r7hzurrs6a2x461/AABySI0lXYNT0OCwuXre_Qh1a?dl=0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-26 06:12:51--  https://www.dropbox.com/sh/r7hzurrs6a2x461/AABySI0lXYNT0OCwuXre_Qh1a?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.6.1, 2620:100:601c:1::a27d:601\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.6.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /sh/raw/r7hzurrs6a2x461/AABySI0lXYNT0OCwuXre_Qh1a [following]\n",
            "--2020-11-26 06:12:51--  https://www.dropbox.com/sh/raw/r7hzurrs6a2x461/AABySI0lXYNT0OCwuXre_Qh1a\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uce1e6283f1490ed45e6f270e416.dl.dropboxusercontent.com/zip_download_get/AnRuENe6eP0d1FlVkdQKa0lD6nGWxz8mhBfjA9KWqzbl1FrYeATYu6BojEd3Rx1NQNUvi-HpjEqp0klaovOtSfnymrQpYJuUF5HzEiPHViOpww [following]\n",
            "--2020-11-26 06:12:52--  https://uce1e6283f1490ed45e6f270e416.dl.dropboxusercontent.com/zip_download_get/AnRuENe6eP0d1FlVkdQKa0lD6nGWxz8mhBfjA9KWqzbl1FrYeATYu6BojEd3Rx1NQNUvi-HpjEqp0klaovOtSfnymrQpYJuUF5HzEiPHViOpww\n",
            "Resolving uce1e6283f1490ed45e6f270e416.dl.dropboxusercontent.com (uce1e6283f1490ed45e6f270e416.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:601c:15::a27d:60f\n",
            "Connecting to uce1e6283f1490ed45e6f270e416.dl.dropboxusercontent.com (uce1e6283f1490ed45e6f270e416.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 438268143 (418M) [application/zip]\n",
            "Saving to: ‘bertmodel’\n",
            "\n",
            "bertmodel           100%[===================>] 417.96M  33.4MB/s    in 13s     \n",
            "\n",
            "2020-11-26 06:13:05 (32.9 MB/s) - ‘bertmodel’ saved [438268143/438268143]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw4VUMfWqKxN",
        "outputId": "f0704c80-8e1a-4f47-c770-bf1c87ecd33f"
      },
      "source": [
        "!unzip bertmodel"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  bertmodel\n",
            "warning:  stripped absolute path spec from /\n",
            "mapname:  conversion of  failed\n",
            " extracting: vocab.txt               \n",
            " extracting: config.json             \n",
            " extracting: pytorch_model.bin       \n",
            " extracting: tokenizer_config.json   \n",
            " extracting: special_tokens_map.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xQG44gPNJ5Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96af6322-16da-446e-aca4-30747b5100fa"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRhApGpDyLD8"
      },
      "source": [
        "#https://drive.google.com/file/d/1r9H92v5A3zDWHzVzI7LFd5PVJA3bcp_F/view?usp=sharing\n",
        "bertmcc = drive.CreateFile({'id': '1r9H92v5A3zDWHzVzI7LFd5PVJA3bcp_F'})\n",
        "bertmcc.GetContentFile('seekers_bertmcc.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFnav0Lzyfxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b28cf63-56d3-4b7c-a96f-c44eb702b73b"
      },
      "source": [
        "from seekers_bertmcc import Seekers_BertMcc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0eK8iALgauv"
      },
      "source": [
        "## **Stance Detection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUWWThsirvGU"
      },
      "source": [
        "#https://drive.google.com/file/d/14AnbACVOTIkBbKzDrQGe0WTdBfkye6b3/view?usp=sharing\n",
        "stance = drive.CreateFile({'id': '14AnbACVOTIkBbKzDrQGe0WTdBfkye6b3'})\n",
        "stance.GetContentFile('seekers_stancedetection.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSGnRVX73RT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90b9b062-a02c-4744-fb18-95104001ce7d"
      },
      "source": [
        "from seekers_stancedetection import Seekers_StanceDetection"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Npl-jpTLHqPM"
      },
      "source": [
        "##**Clickbait**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7kYr99ky_M2"
      },
      "source": [
        "#https://drive.google.com/file/d/1RC8LZBnTR3-kmoxTP_usBcRwTTEqSTZL/view?usp=sharing\n",
        "clickbait = drive.CreateFile({'id': '1RC8LZBnTR3-kmoxTP_usBcRwTTEqSTZL'})\n",
        "clickbait.GetContentFile('seekers_clickbait.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-deoJDyXz1j4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "914342f4-2688-4cd7-b31a-79c5c1f8f29a"
      },
      "source": [
        "from seekers_clickbait import Seekers_ClickBait"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRs9fQZ9H7Bm"
      },
      "source": [
        "## **Spam Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGjPkwvTEJ8d"
      },
      "source": [
        "spam = drive.CreateFile({'id':'1Ku30Kezz0NUzfBlQll8tKVJaFUQAKUpO'}) # replace the id with id of file you want to access\n",
        "spam.GetContentFile('seekers_spam.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Cx5DCzCENDY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13cead78-d72e-480d-f917-1aef636b5084"
      },
      "source": [
        "from seekers_spam import Seekers_Spam"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIy7iPa60JZ1"
      },
      "source": [
        "def isFakeNews(news,source,context,mostly_true_count,half_true_count,barely_true_count,false_count,pants_on_fire_count):\n",
        "    accur = [0.96,0.64,0.76,0.47] \n",
        "    w = [float(i)/sum(accur) for i in accur]\n",
        "    \n",
        "    sumW = 0\n",
        "    prob = []\n",
        "    if ( news!=\"\"):\n",
        "        '''\n",
        "        CLICKBAIT\n",
        "        ''' \n",
        "        prob.append(w[0] * Seekers_ClickBait().predict(news))\n",
        "        sumW += w[0]\n",
        "    if (news != \"\"):\n",
        "        '''\n",
        "        STANCE DETECTION\n",
        "        ''' \n",
        "        prob.append(w[1] * Seekers_StanceDetection().predict(news))\n",
        "        sumW += w[1]\n",
        "        \n",
        "    if ( news!=\"\"):\n",
        "        '''\n",
        "        SPAM\n",
        "        ''' \n",
        "        prob.append(w[2] * Seekers_Spam().predict(news))\n",
        "        sumW += w[2]\n",
        "    if ( news!=\"\"):\n",
        "        '''\n",
        "        BERT MULTI CLASS CLASSIFICATION\n",
        "        ''' \n",
        "        prediction, probability = Seekers_BertMcc.get_bert_predictions(news,source,context,mostly_true_count,half_true_count,barely_true_count,false_count,pants_on_fire_count)\n",
        "        prob.append(w[3] * probability)\n",
        "        sumW += w[3]\n",
        "    probTotal = sum(prob[0:len(prob)]) / sumW\n",
        "    print(\"probTotal \"+str(probTotal))\n",
        "    return probTotal\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnBOM_ZtIkDc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b9a6011-51f1-43f0-b3a2-fed7037f9e39"
      },
      "source": [
        "result = isFakeNews('More than one MILLION marchers for President @realDonaldTrump descend on” Washington, D.C., on Nov. 14.','Kayleigh McEnany','television',16,16,16,33,16)\n",
        "if result >= 0.0 and result < 0.2 :\n",
        "    print(\"Pants on Fire\")\n",
        "elif result >= 0.2 and result < 0.4 :\n",
        "    print(\"False\")\n",
        "elif result >= 0.4 and result < 0.6 :\n",
        "    print(\"Barely True\")\n",
        "elif result >= 0.4 and result < 0.6 :\n",
        "    print(\"Half True\")\n",
        "elif result >= 0.6 and result < 0.8 :\n",
        "    print(\"Mostly True\")\n",
        "else:\n",
        "    print(\"True\")\n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.20613170843744646\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbkZj25ueOjw",
        "outputId": "668db2a6-313f-4a49-9a13-eb02a130f41f"
      },
      "source": [
        "result = isFakeNews('Media fawns over Biden\\'s Cabinet rollout, describes being rescued from this craziness by superheroes Members of the mainstream media showered President-elect Joe Biden with praise Tuesday following the rollout of his foreign policy and national security team. Biden formally introduced several Cabinet nominees and White House advisers, including Secretary of State nominee Antony Blinken, National Security Adviser pick Jake Sullivan, U.S. Ambassador to the United Nations choice Linda Thomas-Greenfield, and newly minted Special Presidential Envoy for Climate John Kerry. MEDIA OBSESSES OVER ANTONY BLINKEN\\'S GUITAR SKILLS AFTER BIDEN ANNOUNCES HIM AS SECRETARY OF STATE NOMINEEDuring an MSNBC panel discussion, PBS NewsHour White House correspondent Yamiche Alcindor appeared to agree with what she said a Democrat source had told her. I was talking to a Democrat who just said this also felt like \\'The Avengers, Alcindor said. It felt like we were being rescued from this craziness that weve all lived through from the last four years and now here are the superheroes to come and save us all.','fox news','in a report',0,50,50,0,0)\n",
        "if result >= 0.0 and result < 0.2 :\n",
        "    print(\"Pants on Fire\")\n",
        "elif result >= 0.2 and result < 0.4 :\n",
        "    print(\"False\")\n",
        "elif result >= 0.4 and result < 0.6 :\n",
        "    print(\"Barely True\")\n",
        "elif result >= 0.4 and result < 0.6 :\n",
        "    print(\"Half True\")\n",
        "elif result >= 0.6 and result < 0.8 :\n",
        "    print(\"Mostly True\")\n",
        "else:\n",
        "    print(\"True\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.213009173833498\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxCJ9XckJ-5i"
      },
      "source": [
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAW8AAADCCAYAAACGyUOQAAAgAElEQVR4Ae2dB5gURdPHBckKSM45g4AEJYjkHCXnDCogOccjSM5BQLIIApKRnDMcOYogQUCS2RcUEbC+599YY+/s7M7u3Y7f7V3t8wwz09Pd011d/evq6h7upZfkJxIQCYgERAIiAZGASEAkIBIQCfwHEiCiOTfv/05nrv4ih8hAdEB0QHQgAuvAvZ/+ICLqroYGIpq09dhd6jnzjBwiA9EB0QHRgQisA6eu/Ax4t2N4hyzedoMq9tonh8hAdEB0QHQgAuvAntMPAO+GAu8I3EgymIoxITogOmDWAYG3QFusK9EB0YEg1AGBdxA2mnkElnuxykQHop4OCLwF3mJ1iQ6IDgShDgi8g7DRxMqKelaWtLm0uVkHogy8a/Q/QC1Hh1L9kMOOWBkLNl+nK7f/R+eu/UrvDjroyDvMjSf30qFFB6KuDvgM76YjjtKjP57So8eej98e/UWAZERSqK7TT9Glm79hS43xw+b2AXPPBbScB879YOTfePiRgOYdkeQpZYm6sJC2j1ht7zO8248/bsDJ08Wffz2LUPDGgPPwj6eWxd105G5AASvwjliKLaCR9ojsOuAzvGFRL93xLS3dcVMdTMS///7bCMMHPpV6v1AauCjen3iCqveztsQbDTtCHSefpA8mnaB3B7q6GZCGLfiqffdTh0knqMWoUAO2eIawZh8dNcKsGmrKqitcTLp65yENW3SB5m68Rg9+fkxr9n/nltauzHiHpzhW8K7SZ7+qW+U+rh3JUx6Ve+8z4tcaeJA6TT5Jjf6x4iHX5iOPUqcpJ6mpTb2tZCFhrm0g8hB5BLsO+Axvc0WfP/9bgfHH3/50geC01VeUa4WpiXi7Tj4wAD14/nm69eB3fqzOz579TesPvoBpm7HHjGdHLv5Ij588M+7PXv2FZq2/Sr8//teahp+54TBrP/ayXTeNtICruQ58b1dmxLOLY4Z3v0/O0l9Pn6v33/7+d/Vuuzz+aQxV518f/aXSQr49Pj5N//w/BkZ9Lt74zWN9uF5yFkCJDkReHQgovEcv/cqAi/li4+E7CjYnvv7J/Mi4h2X83gR794yR4J+LtQfcrWgo7ZSVl12iwvcdsvCCC/R8KbMvcXR4wzr+5eET492r990mf/PgxBjofvz1T741zk+ePneph3TSyNtJpW2lba10IKDwxv9IiN+z539Tr5lnqMu0U4bP+clfz5VLZcXuW3T8659o6MILavqvW8dwyZjhvffM93RQWwxE/icv/0xbQu8aILt863+WIKs96KAl+L6+9T/lgoBAfCmzL3F0eOszi8MXfiS4Q/zNA+sHmI2sO/CdUU/kCzcSLPE5X16zrLNVI0uYdH7RgcinAwGDd7W++4ldKaDN7lMP1GGQh4h4FwZ8ubM3XCWA/OhXPxpRYEHr8AbkoXTw8fLvxr1HBF8y/MgYEPC788MfHkEGP7GVtY98avSzLzPS+1IvHd5c1rs//qF8977KRs9j5GdfqTqhrrCy+Xfvp8c0f9N1gk9cOmTk65DSptKmvupAwOANyNn9sPi2ZPu3HqOZ4b356IsdIVig5B+sbq4cuyYASQ7zdO489RSd/kb9F4qcFcEvbff7cOopuyhqUVEHLyf46bcnBOvfV9noefBAh/rAyjb/bj74nWoOsF4M9iQDCRcwiA5EHh0IGLwBEv49ffacYDnqB/ZVA8JwqeAHtwBcJrrbJNDwxgc5+gczcF88+OUxF1Pt5uAbT2X2pV7oEDp4sReef1tC7ynI8r2n95jz0OGNZ9iZs/34PUJ6/o1YfNF20JLOGnk6q7SltKWuAwGDNzKFpck/7AppMuKI2i4Iy3HqqivUesy/O0mu3XlIDYYepg2H7nASCjS8Q7/6Se18+WLPLRq66AIt3HLdxQVSd8gh2zL7Ui/E0eGNQQtbKPnXZ/ZZn96j58HwhuW+/+z3aiCEq0T39Q9ZcF7gLf+9gehAFNWBgMLbanrPADt//Ve12KZv/eNnfA40vI9d8ryzBRUHdO3K7GscM3jxERD/4NaZv8nd9cHPIRvzAMDwxp5wqx/88PhwCunkEBmIDkQ9HQgzvP/488X+a93fjMU1fKhjBjTu+S/0jFrylfrMnoGkW+ur9t4m7PNmq3XdP3u/q/Xbb7gLsPebFRUf2+D37f1HRhg/wxmuGuyx1n9Y5MTWPf4IyJcy+xIHe9nxA1ThroHFzC4auIrwUZKdbMx5oA74YAl72fUf/huCSV9ctqyzXn+5jnodWto86rR5mOGN3R7wB8OPbFYYfA2Irx/hp4XlaP7CEDBuN+44tRrz4qtJwFFffMPuDP0e+XMc/oITYd7KoJcJC6X4ItOqLBzPrsyIZxcHZcZWPj1PhKE+epg32Zjz4HSAP+rQanSokgWHy9ld/0QmIpOooANhhndUEI7UUSAgOiA6EFF1QOAtPmNjVhBRlVTKJQAVHXDXAYG3wFvgLTogOhCEOiDwDsJGEyvE3QoRmYhMopoOCLwF3mJ1iQ6IDgShDgi8g7DRopqFIfUVq1p0wF0HBN4Cb7G6RAdEB4JQBwTeQdhoYoW4WyEiE5FJVNMBgbfAW6wu0QHRgSDUAYF3EDZaVLMwpL5iVYsOuOuAwFvgLVaX6IDoQBDqgMA7CBtNrBB3K0RkIjKJajog8BZ4i9UlOiA6EIQ6IPAOwkaLahaG1FesatEBdx0ww3s4/iwZ/nSYHCID0QHRgfDoQJPhR4QjDrL0wLnv8d/8N3kJP/zRd/0//ZdrkYBIQCQgEojQEpgi8I7Q7SOFEwmIBEQClhIw4L39+fPn9OyZHCID0QHRgfDpAP6cocgwfDL0Jj/8yUUimsCWd8jdBw/p9MUHcogMRAdEB8KlA4C3sMQ5lv78q/obvg0F3tJRpaOJDgRUBwTezoEbg6LAWzpsQDusWFrOdthgkq/A21ldEHgLvAXeogOO6IDAW+DtiGIFkwUjZXW2E4h8nZGvwNsZubK+iuUtVpcMjqIDjuiAwFvg7Yhi8eglZ2cVTOQbdeUr8Ha27f22vId9NJUaNG7tdgwfNV0gKxac6IDogKED/sA7ZMRkql6zPtV4tyFNnLrAyEMGf88DgN/wTpsuA+FLevNRomT5gAi8/+CxVKBgEWrRqkNA8pPG99z4IhuRjZM64Cu8j566Sa+88qrBlNRp0kvf98EI8BveadK+gHfGTFmp7XvdjCNQo2XN2o1UI+bOk18a0IcGdLLzSd4C9/DogK/wnjBlgQFuNgo//XyT9H+b/h9meFeoVMNNuBhB+w4YRSXeKUdZsuak7DlyU41aDWjNlwdd4u47fJmat+qgLOzsOfNQvQYtaeuu09Sq7YeEURcNGD9BQqpSrQ6937GXkXbhZ18S4J7n9TcoX/7CVKt2Y1q1fp/xHIpWpXpdKl+xOi1aupEKv/U25cj5On25NZRWrttL1WrUU/c5c+Wl+g1b0v4jV1zShkdRJa2ATnTAVQd8hTf6K/o8ZtwxYsZU142btnPrm564wXKfMmMxlatQTXGnSNGSNHLMxyqP8ZPnU6kylahZyw+MPNdvPkply1dVrNh76GsV7okdy1btVLzIk7eA4lrRYqWo36AxdPzsHSM/lMHq/QuXbKTSZatQhUo1XXjz8SfLqXSZysr9zOX39xxQeAO0PHLq58RJkhkFRwMkT5HKLV6DRq0oZao0buGZs2RXAhoUMp6iRYvm9hyNPWfBakOI/F5WAtxPmrqIXo2fQKV9+eWXjXxmzF5mpPNXcBLftaOKPEQeZh3wBd4Hj12j2LHjqL45YvQMgvsVfTZJ0mR04txdo3964wbe263nEDc2IJ9dBy5Si9Yd1bNUqdMZ+X0yf5URf93mIyrcih1zF64xBhR+zufO3QYa+Xl6/6Rpi4z3DB46wYhfvEQZFR4eD0OY4Q3QwpLlY/b8lYRFh0pV3lUw3bX/AqExuKLTZ3+uCg6LG2EAcdMW76u4lavWpvc69CTkgdEXzyFojJxzF62lPYcu0auvxlfhGP0WL9usFjUYyLDEWXH4fThjtGvS/D3q1Xe4SgugQwkOn7ihyrZ5x0kjHaeXs0BIdCAwOuALvJkR6Jv7j35D2PjAfXjWvC+M/umNGzv3nac4ceKqdJmz5KBxk+bR4GETKV36jHTo2HW/4a2zY9P2E1SwUFHq2WcYbdgSSmDG63kLGiyCrnh7/8HQa5QqVVoVv9CbxVV9DoReNQaErj0GG3X0V+/CDG8WMJ87du5rFALuiNUbDhDcHPx8yPBJ6jlGGoTBdWFVWCufN6YjnA/Azelat+uswqNHj07Hznynwjme7taZNP3f0Q/TJwwInIecA9NRRY4iR7MO+AJvuFjRZ0uWrqj6JNjBs2a4RTlPb9yYNnOJwQewgtPw2V/LW2cH54FZAMC99IvtyhWMMsODgOd272//QQ+jfNt2n6HR42cb95u2H3crL7/T7hxmeMOf3af/SOMArLfsPEX5C7xpFIxBijNGQhSGrWW4SawKZwVvtpyRD6ZZnA558jsgFITzPcrG8QD2QoWLGc8Q542Cb9Hug18ZcTiunAVCogOB0QE7eGNGDTcm+iNmzw2btFEHMwLn0NO3VB/lMCtu9Oo3wujbcJOY289feOvsQF69+3+k3DjMFj4nS55Svcvu/bDYOU33XiHKI4B7WPDmsvpzH2Z4W41OWCBEoeCv6tS1P2FPOBea4Z0sWQoVBkhbFZThnSt3PuM53DGcj+7q6NYrxAjHYiny43jmBsDIOWbCJy6DS5Nm7Y13WJVFwgLTiUWOUVOOdvAeOGSc0V+535rPk6YtVH3UGzfgS+Z0MCDN+tayTSf1HGtv/AwuGU5j9nnr7Jj36TojHhZWUeYy5aqqMIa33fvxTnYHYw0vXrxXVPoevYca5eFy+XMOKLy5UJgmoBD6iMPwht8HQkuQ8DXauO2Yijdq3CxlwSNN3QYt1HPsNoG/Cv7pBYs3qDCkg6sE8eA3wo4WhPGiJsK5QfQG2LTjhNpxguc4eHaAVWAOk3PUBIy0u3Ptbgdvng2j/w4YMtY4YOny5gSsW6GNvHEDblDu91hHQ/zte8+p3WhY49IXE+HKxe6zrNlyGWm8wRuARd6YIfAsAB8TIYzhbfd+lAduYy4jn7HDLjz6F1B4w4+NgmFxEaMTAMwFZXhj2w6HwVfNC5HYUoiK6NZ0zJixKFv23CqcFwmQFouZPI3CPRYoWAictw5v5Il3QQHg88Y14nXq0s9Ix+nl7FxnFtlGLdl6gzfcnNxXAVezbrClip0oMOK8cePk+XuKE5wfmMJ9HD5l3fjjOPrZG7z19ba8+QpR/jcKG+VmeNu9H3WDu5cXVfFuGJDmOvt77ze82dpl2OovxMb6FClTG5VDXPiyUNhhI6cZhcVoFiduPCMepkTTZi1VzzFS8iCAdHDF4B1obCxq8IiMZ0mTJidY7XoZYsWKrfLVt+Wg8RIlSmK8Dw2L8h87fdslrZ6PXEct0Eh7B769vcEbBhf6MCxanoHrbYAvrfEcB28w8MYNzPJ5Ro004AT2fPNGBuxWYXbga05sPcbsH3F50dCKHacu3KfadZsaZYkRIwZVrV5P3etfgtq9H3XDp/9cJ9241Ovtz7Xf8MYoc/j4dY/Qw8Z1jGTse4Kv2So+8sFGeRwQkF5o3GNKsW7TYbeN8PBt48McbEXU0/A1GguuFr7nM96HcuFjHX3Rk5/LOfCdV2QatWXqDd7QDXDBmwGFfmzuy964gTyx3RB93OoDPFjwq9bvN/aP4928Voa0ntiBZ1hcRb7sOkE6HhjwnA9v72ffOwaRHXvPG2k4rb9nv+Ht7wsk/r8NK7IQWUQlHbCDd1SSBUCfKHFSZXmzNyG89Rd4a6NmeIUp6QXOogP/6oDA+19ZTJ/9ueEy4W9ewqsrAm+Bd7inb+FVQkn/byePTLIQeP/brnDRfLF2j+FfD0Q7C7wF3gJv0QFHdEDg/S+8AwFrcx4Cb+m4jnRcs6LJvbMdOSLKV+DtbJsLvAXeAm/RAUd0QOAt8HZEsSKipSJlclbZRb7/rXwF3s7K22x5b372/Dk9fSaHyEB0QHQgfDoAeIsMwydDb/J7/vxvIqIxL+FHRKN///0v+vGnP+QQGYgOiA6ESwcAb2GJcyz988+ngHcLhnfIw0dP6MEPv8shMhAdEB0Ilw4A3sIS51j6+AW8Gwq8paNKRxMdCKgOCLydAzcGRYG3dNiAdlixtJztsMEkX4G3s7og8BZ4C7xFBxzRAYG3wNsRxQomC0bK6mwnEPk6I1+BtzNyZX0Vy1usLhkcRQcc0QGBt8DbEcXi0UvOziqYyDfqylfg7Wzb+21533vwkGbNXkj16jWikiXLUJOmLWn12s1BDdg167ZQ6zbvqaNN2/dp0ODhtPDT5XThqxtBUa9167dRtmw5aMbMeUFRXgG6s506osjXF3jfvf8/mjxlJjVq3JwqVapKvfsMpDPnvvFJj1eu3kjor2XLVaSq1WrS2PFTLdNt2rJH5d+yZVv69taPlnEiisz8KYff8G7Vur36f2mTp0hJhQu/RfFeefGXkK9cvaOEsmLlBipdpjwdPnImaISERsU3SsmSp6BkyZIb/+9u/PgJaMLE6X7Xw0kZYNDs0bOfS5k+mfOpKnPIsFEu4f4ogsSNGkD9L9vZDt4Ad81adZTupkiRkvLnL6Cu06VLT6fOXPaqy23bfaDi4u9VlihRivLlL0AJEiR0S3P7zi+UKXMWFRd9/PzF625x/kuZBPJdfsH76vV7SggQ1K3vflZCuH7jAX00cjx9d/dXdT9m7GQVB9ZgIAvqZF4M7+Mnv1Jl/ubaXZozdzElSpRY1WXBws/9qouTMkiSJCm9/XZJt/Jcu3HfLcxJmUneAns7HbCD9+IlK1X/gkHCeY0cNUGFdfqwuxHGz/i8YNEyFadc+UoulvTN2z+5pRk4aJiKi5lplIb3/oMnlACq13jXTUgQLARfrHgJFQfTmHbtO9Dny9aouBjx4I5AeIWKVej9Dz4khiXShh47T82at1bnqdNmU42atdVUB1MjbjQ+79h5UOVdvkJlatGiDeGen8GtM3HSDKpXv7F6V7/+QwiDDj+3OpvhzXF27jqk6pImTVqClcDhh46cpo6duql6tGzVjr5Y9aXxzJsMfCkbLIURH42jWu/WpapVa6hrKOWRo2dVnWPFikUpU6ZS15DhnXu/UejxC0p2m7fuNcpx//tHyo0CmULeH3ToTNs1OaEuw0eMVflD9h06dFHT1l69B9D1b7838uE6y1lg7a8O2MGbre4Tpy4Z+gYjEDPeDBkzGWHm98LSRj+4eOlbj3GQ5vTZKxQvXjzVl9hjEGUtb4AF0xT8AU1ACjDSBVunTgOCZYgRLlWq1JQlSzZllSNO5ixZlcBhNeJAHMSFlYvnq9ZsUmE8xcFIiQbCu9Zv2G68B75o/MVp/AVouG0wVcJfdEY+ABb8X8j7rSLFlB8Y18jzxs0fjDz0MuPaE7zxjPMD4LiccePGVWUvU7YCxY4dW71v/D/uFU8y8KVsGCAKFXpT5Yf658qVR11joIDfDvJEffBOXOfMmZvQJiw7LgPeBf8h4qZNm07liTSQG1wsXP8iRYur2UXChK8RDsRFmlKlyhpxOK6cBd7+6oAdvKG/cL+a8y38ZhGlq+Zw3KOPxIkTR7lmcX009Jwy+MwsQlwYQIA3IB7l4Q2BwJ0QM2ZM1ckBkOkz5rhA3JPLAItp+kg5YOBQlceixSsMKAIcGHU3btqlwmC1IwyLGdxwAAwa5NiJiyoMVnXXbr0J1in7fmE9Ij4Ofg+sWQ4zn73B+8POPVQZln6+Wg0O2bPnVH5xHsEvXb5F8NclTpzEsM6tZOBL2TBjQH2haFxG+M8hc763cpuY4c3vwuyDZwx79x9TAx7KCuAjP8Ab76tbt6EKgxX/5ltFVdjJ018b7+R3y1kA7o8O2MEbugyAm/OsXLma0kE27PTnWMyEzsKl+dpridQ17sGE0WMmGXlhEwLC0f+RXuD9DxD37As1LDsICLtO2N9kBS5d+LgGcOFHRtpxE6Yp4TKA4D/X48PKRf4IwyiLNJ4sw4aNmqnnW7bto4OHT6lj2fK1Kqx+gyYu+erv8Abv7j36qvSfLVlJmN7h/bXr1Dfyx3uqVKmuwnlAsZKBL2UDRJE/D156GfnaF3jD+kc+ZgBjEEQ43F/ID/DGoKNbLYOHjFBxvty406O8uCxyFph70wE7eAO4ufPkddMzuFahp5e/+c7tGVyWeIYZd89e/ZXLcsbHc5UrEeG79xxRrkQMCphxs6Ei8DZ9zACfMC8EYGEADWkFLoTD3wo3A6Y8EDIfvL2H4c1Tf1YKTKsAGdwvX7FOpdMta46Hc9Fibxv5cv58xhRKj6tfe4N3g4ZNVZ4HDp1UWyI5P6szFAv5WsnAl7LBDYRZDSucXka+9gXecL1AzpyGz337DVZ1gTWPMMjVPG2F/FE3tAenk7NAOiw6YAfv1KnTEHaWmPOGqxLuUt2o4DgwkKCfZmNsytRZKhzrOLDAEWfm7AVqAMAgwIYLjC3ebMF5BuvZr90mVpXcun2/EhRGSzy3AhemPxgpsRUPbhaAnIXtD7zh+0ajeII3FubwHP5pbDXSD54ZWNXBE7yxcIetg/C9o8FRbuSPRUo9b1yzG8WTDHwpGxZiokePbrg6rMrqC7wxU4Hy8w4gzoddQLzAK/AWKLNuOHG2g3fefG+ovmXee43FSrj3rMqEXVXog6VKl3N5zi5WbIqoVr2WioN4Voc3Q87qnRE1zC94K0hduOYiNOwGgYDwkQsqyZbbvPlLjHj4iAdxMM1hQWDhEWH+wPvc+asqTYEChYx8kB/8ufDXYkcF8oSrhN/jy9kK3oB94yYtVH7duvdR+WEQAhQLFizsNX8rGfhStuYt2qj38Q4dlB3lgLXA9YClnCNHLuMe4eZZC9oCctC3OML3nTVrdoIbCts7kU7gLfBmvXLibAdv7HCCnuqs2LbjgArDLjIuE7ij+78zZMioZpb69tguXXu90PlFy5TbcdLkj0k/eBccvoXYuHm3kTe/IxjPfsEboxo6P7af4eMVLAYkTZpMrQyzQNjHDMAhDu7ZVw0fFIAyavRE4+Mef+ANAfMuCmwzghUP8MJahX8XW+bgdoAfF1MnDCyIg5EYLhdPDcTwhjLhCy+4SqAgUCysfOuWQdNmrVQ4tjJisRWgxaCkj+ZWMvClbLCI8U5Y1/D9YxYD4PKCLcoPuaK+/QeEKPnDujbDG7BHHCz+hgwdqaaPWCdA3vDhsxwE3gJv1gUnznbw3r33qNophp1O2FAAXmBbLnZG7dp9WOkpfNjQW93SZvcfXJGzP1lEMK6wkwobKMyzTa5XlPd5A0AQIsAAgeLImCmzghgLCRbwO++UNp7zai9gw7tUMADwFJ6/YFy7bqtKg9GS88IZfrHixd8xwvDJOvvE8H40ND6R5V0V2HONbYlcPpyxeAFF0fPVr+GG4fioW/r0GVQ9sfsD2+70uBjtYRXAlcJpsG0RljXH8yQDX8qGxRd9FR1WNg+MyB8Lp1jowbtxhrJayQ4DCOrBZUQHwcCk+xEhV2zp5HLjjE+VkQZ56uFyLaD3Vwfs4I38lixdpYDNegpjRf/vNrDWhNkuu2WRBv0LRhTCkQ7gBpdgoXsqI3+Rqe948xQ3WML9sry5UvAF46MRbJPjMPMZkMX+Sj0cFiz80RA+wnGvwxH56veIA1+z1WjKZfDky8Y0C5/o69MtvSzhvcZggcUTWPw6EPV8rWSA576UDfmePX/VRX6cN2SCgVTfu24lO8T/+sptj0oNuZoXbyB/5MXvkrNAO6w64Au8OW+sGUFX+V4/s5tPD8M19B+zerMOm+PhHn1Un0FbxQm2sDDBO9gqKeUVAIkO/Pc64A+8pX38bx+Bt2n7oyiR/0okMhOZWemAwNtZvRB4C7wtp6pWnVHCnO2MkU2+Am9n9UXgLfAWeIsOOKIDAm+BtyOKFdmsHKmPsx1F5Ou/fAXe/svMHz0Ty1usLhkcRQcc0QGBt8DbEcXyZ4STuM4qocg3cspX4O1su5ot7xV/PP6Lfv71sRwiA9EB0YFw6QDgLSxxjqVPnjwjIhr0En5ENAPCvnH7VzlEBqIDogPh0gHAW1jiHEsf/f4E8O7I8A65++Ahnb74QA6RgeiA6EC4dADwFpY4x1IY2kTUUOAtHVU6muhAQHVA4O0cuDEoCrylwwa0w4ql5WyHDSb5Cryd1QWBt8Bb4C064IgOCLwF3o4oVjBZMFJWZzuByNcZ+Qq8nZEr66tY3mJ1yeAoOuCIDgi8gxDeO/edp559htG6TYfDpBThTc8jk5ydVR6Rr8jXmw4IvJ3VjzBb3qvW76cPuw6gCpVqUPmK1alL90EE6KIxP5m/Sv2Fi0Eh49X9vE/XUabM2eijMTN8grk5vTcFwbPjZ+9QyzadqEHj1upo3LQdde42kDbvOOnT+/T8q1SrQwUKFqGT5+/5nVbPR66dVVyRb8SXry/wPnHuLoWMmEw1azeiUmUq0QedetO23Wds+97MOSuoQaNWVLR4KarxbkMCY6x0Yvb8ldSwSRt6u0RZKlu+Kg0YMtYynlXaiB4WJnhDADFjvvgzYImTJKNEiZIoWL8aPwHtPviVG7zHTPhEPe/eK8QnwfkLbwwa+MYobrx4lC59RnrttcTqPkbMmITG86cRMmfJQagHw3vm3BVU7O3SYZ5F+PNuiRvxgSRt5Hsb2cEb4K5Qqabqq8mSpaDcefKr61Sp09GWnac89ttOXfqpeEmSJqPCb72t+j36/6Tpi1zSNGraVsXDnyl8s0gJypU7n+rbkaUN/Yb34mUv/hJ80qTJaeGSjYawFizeQPnyF1bWrhV8Dx67ZsS1E55Vem9pGN7v1mlivGPStIWq4Qq9WdwI85YHP4NCHTt920jTb9AYlY+nkZ3Tydn3Ti2yihqysoP3lBmLVd+qWv+utLgAABkpSURBVL2e0d/69B+pwjCTttKTbXvOEowyGFkHQq+qOF+s3aPCMmbKaqSZOHWByqfEO+XoyMlvjfCjp24a11b5B1OY3/AGDDHKffzJco9CMMP3y62hVLteMwL4deF8vnIHwcVRomR5qtugBeEez83pT124r9wgdeo3NxpMz8cK3nj+6qvxKXWa9C7vnD77c0I+xUuUUS6fYR9NJeTP+fXu/xG1fa+buociFSpcTNUXUy6UFek5rpyjBoSkncPWznbwZqt7044TRp86duY71W/TpstghOnyZ+D36D3U5XnJ0hVVP2WLHZY2vAO7Dlx0iafnFezXfsEbvmX8pWa4JrxV3Axf8z3SwjJGXvHivaIsdrgqYsSIQfuPfuMG72YtPzAAyu4M/f1W8P5s2RaVpnSZykZZR42bpcKgGO+UqkApUqZW95269jfiwN+NWQXyh/+bXULJU6SiDBmzEOCuv1uuw9axRW6RX2528M6SNafR13R9yP9GYcUGPYyvR4+frfosDCsOw7l1u84qfO7CNYTZc+zYcZS7E9frNx+lDVtCDVeoni6Yr/2CN4QAqxvg81ZpM6zN9xBoqlRpKU7ceLRx2zGVF6ZAbdt3JUxr9PhdewxW7yxTriphVLZ6L8MbeWLxtEjRkqrxYHnD6uc0qzccoBmzlxn3cOUkTJiIsufMY4Tp8EY6cZtEfsiwfsg5sG1tB28YRgC4We4wuMAZGHLmZ8tX71bP4L9mtwnO5SpUU+HTZi1VC55Ij76dIOFrKhz34E2/gaPd8jS/I1ju/YI3fNwQQpXqdb0KQIcvBGG+50GgaLFSlvlwfIzAeJ83cCN/hjcvYOTNV4jiJ0io0mL6pPuwuWEwEOw5dIlez1uQkI7DBd6B7cAsVzlHPbnawRswzZ4jt9H3WEfgokS/33f4stszxGF3C2bt6dNnUv5uxMeBXShrNx5S15jNv9ehJ82a9wWNGD2DkiVPqcIxAPC7gvnsF7yxhQcCKlioqNfKM3x5q6D5Hv5y5PN+x16W+XB8bpBefYdbxmPBM7z1BUssUrAScDlg8bdp38Vwl3D+2DHDeQm8ox5kuO3lHNi2t4M33JbYWWKWO7b1RYsWzaObA2tU4yfPV325SfP31PY/rJmhPwPMmM3junrN+i55Dx0xRYXb8cRcnoh67xe8ITT4kjBiwmr1VCmGL0PTfD9/8Xqf4A1fNHzNWF02L3bq77aCN55jFEYjYp8n7rGHlBsVvjGsUmN7ksA7sJ1Wbxu5jrqytYN3zlx51aKivhsE+oI1KWwd9Ed3sGUwVqzYyrUKdyj6OfaA63lgswHC8U2KHh6s137BG5Ws37ClEgCsXPPi4bhJ8yz3eZvhvX3vOZVHnrwFXIS4ct1e9cGNHn/R0o1qIROjNPaQWwnaE7yxIo3GYgsfM4bo0aMbvnOUP0fO173CGwMQ8kDdrN4tYVEXTtL23tveDt7NW3Vw61tLVmxTYbCkWb7YQWLl/+bncIugj+qWdpq0GZShqW9Rxqwb8bCNkNMG89lveAOUsFQhBPiL4VPCgVEUYSvW7HbzceswZmHhayrEh/9q+KjpVK9BSwVWfBVpjo9pDuJiJDUPGMiP4Q0ruluvEDWy1qjVQEEfi5a8FQn7SZEPlGbazCVq4RX33ixvLHAiDuo6eOgElwVProucvXdikU/UlI8dvMEK7DDD+lSvfiNU/0qZKo2yoJev2qUAywuUbEVj/QqQHjxsonKdYD84tgRi8ZP7OfStY+e+qt/CYMMOFWz/xe427BjztPEh2PTUb3ijgoAldnUAjAAbDmwf7DtglNozPXfRWhU2ZPgk1QDme+Sxa/8F9ckqfFtIjykP3BvwS1vFx2f4iIeGMAv50LHrxldWXB6sMpcuW4VgzXN8+MKyZsul8kE8rFhj54w+RcM+drhqOA22R75V5B0jDf5LAH4m56gJJWl339rdDt6QI4woAJv7LT60mbNgtdHHsEMMjMD6FeIDvGANx8dMGl9AI57eLui3tes2VWkRF+DGAMD7wPW4wXodJnjrlcUiptWq8OHj110+fjHfcx4IX7f5iNoiyGE4W8UHpPUPavT4/lzjKy0MHkiDwSL09C2j4aEc+j3ni/hbd5024nG4nH3ryCKnqCcnX+DNegGDcO+hry37F/o9x+Mz+jB2rVntJOM4OB8+cUPFs+rTerxgvA43vIOx0lLmqAcSafP/vs39gbe0j//tI/CW/8vZzaqRjuR/RxKZuctM4O0uk0DqicBb4C3wFh1wRAcE3gJvRxQrkCOg5OWskop8g1O+Am9n200sb7G6ZHAUHXBEBwTeAm9HFEusOWcVS+Qr8hV4O6sDYnmL1SWDo+iAIzog8P5v4T3vt4d/0p37D+UQGYgOiA6ESwcAb2GJcyz94/FfRETdXsKPiD598uQZPfr9LzlEBqIDogPh0gHAW1jiHEufPn0OePdmeIc8fPSEHvzwuxwiA9EB0YFw6QDgLSxxjqWP/3wKeDcUeEtHlY4mOhBQHRB4OwduDIoCb+mwAe2wYmk522GDSb4Cb2d1QeAt8BZ4iw44ogMCb4G3I4oVTBaMlNXZTiDydUa+Am9n5Mr6Kpa3WF0yOIoOOKIDAm+BtyOKxaOXnJ1VMJFv1JWvwNvZthfL+x+r6/zF6zRs+Bg6fOTM/9tgsWbdFuo/IIQ+Gjmert948P9WDgGus50uqshX4O2sHvkN73sPHtKs2QupXr1GVLJkGWrStCWtXrs5KECz/+AJGjBwKNWoWZuq13iXBg0eToA2OtOqNZvUn1YaP3G643WpU6cBFSlanCBL7sjTZ8xR748ZM6b6k00oj1U8ji9nZzuGyDf88vUF3nfv/48mT5lJjRo3p0qVqlLvPgPpzLlvjH5hbofzF65R6zbveTx27zniknbl6o3Upu37VLZcRaparSaNHT/V5bk5/2C69xverVq3V5BJniIlFS78FsV75RV1f+XqnQgtFDRarFixVFmTJUtOSZIkVdcJEiSkr76++Z/CO0eOXIT36vAuWuxtihcvHl289C1dvX6Pbn33M1nFCyblkrKGH4DBLEM7eAPcNWvVUf0wRYqUlD9/AXWdLl16OnXmsiVP9h04rpiDvqIf/DctP54130jXtt0HKj/8rd0SJUpRvvwFVL8LZpnqZfcL3oAKhAQhAC7ICNN7TPO/u/urITT9BRHhevPWvarcGHA2btpllHPDlzvUAHTy9Nf/KbyhtLfv/GKUAzLKkDETvZ43v0uYVbyIIE8pQ9SGsq/tbwfvxUtWqn6JWTznOXLUBBXW6cPuRhg/83YuVOhNNWNlq33BomUqn3LlK9G3t3408rp5+yfj2lt+wfDML3jD7QB4w+XgqXLDR4ylMWMn05GjZ6llq3ZUvkJl6t6jL1346oZLms+XraHmLdpQmbIVlBtj6rTZdP/7R0ac0GPnqVnz1nQ09BxNmDidqlatQfXqN6btOw+qODNnL1DTrIaNmhF8xZ7Kg/DixV/89fflK9Z5jGflNrErIyx21K1ixSqqDvqo7+0ZBrtu3fuoshw7cZHate+grIikSZOpa/jeUW49Htfv0JHT1LFTN6pQsYqS7xervnSpE+Q/4qNxypWFaShcL+aBgvOSs0DYSR2wgzdb3SdOXTJ0GEZg/PgJlDHja9k2bt6tuFTr3bpGPrC0MdPGTNbXfIItnl/wBgQwBYkWLRphhNSn/Vxx+HITJ05CcePGpZQpU1HGTJmVYDNlzkLXv/1eCRI+cwwCsDYBodSp06h7LNZxPgxTxEHcbNlyqDOmStzombNkJfYR79x1yEjLeeB8595vakRGOfRw8zW/j33edmWEVZwzZ26KHTu2Anex4iWUXDAAeXuG90JGmAXgesfOg5QlSzaKHj26qguueXDU4yEuygi5wuWDQQ/vhmy4zJw32ihGjBiUMOFr6iyLnwJp6MZ/fdjBG/2H+4FetsJvFlF9Vg/zdl2tei3VDwBxxEP/ixMnDpUuU15dwwCEMWjFK2/5RvRnfsEblZkzd7GCDKAB0GChTRcKgINnWIBgV0rTZq1UGCxo5HHg0ElatnytoUzXbtynRIkSU57X8xlhDFP4hrdu36/C+/UfovIBmBZ+ulyFzZ33mQr7oENnI60udDQcyoNBQg83X/P7GIR2ZTx4+JTKt0fPfka+cL8gX2/P8NwMZYSlSZOWChQoZORljodBIXv2nAR/PS+yXrp8i+ArxGAJheU0qC+sbri2ItM0EfWTI3hkYAdvGCEAuLlNK1eupvrWN9fuuj0zx8XMFYaP3nfgOkEfAFNeey2RusY9DL/RYybZ5ml+R0S99xveqMiefaEKDhAIDuw6YUgATICJDnTERzwA3SwIAB4QKliwMCVLnsJ4zjCF64DTYLEC+WAhgsPwXoRhBwmH6Wf4uPG8bt2Gls85Lr+P4c3hOFuV8esrt5V1gMUV824bb8+QX1jgjakl6lG7Tn01OGCAwFGlSnUVDiXmvLGIHNEXkHX5ynXwANmftrKDN2CaO09et36JXSHQ9cvffOf2zPz+9u91VHHhRuVncC0iPQy/nr36E1yLMz6eqzwBCDfvSOF0wXYOE7y5knBVsDtj4KBhBjzMUyFM2yE0+IaRFlZil669DHcJnuGAVcl5W8EUli3iwefL8XDGyAufuB7G1zwKYzcHh1mdze/zpYzY4gQ3BsoEC2Lp56uNd3h7FhZ4Y4BgOVmdobCol1XeVvWVsMgJzIjUrnbwhrsUxo+5zNjWB9esbgCa4+AeljkMFfCGZ/kIhyGDPlK/QROXvKdMnaXCsS5klV+whYUL3qgsXBoQFEZL3FvBgwHKwsReThYuFhthmWObkBPwhrsB/i+M8rDwPTWQGd6+lBF5YWES+8XhX8Ygsnf/MeMdnp5ZycjObYKFWsgMi8DYRqUf7EbxJH9PdZZwAbiTOmAH77z53lCLivpuEJQH61xwB9qVLWToSNUn+vYb7BIXblj0lVKly7mEYwMCwtFf7fIOhud+wRvAwCZ5vWLYBA+BYOM8wgEmLCKuW7/NiDd23BQVhxckYQUDdDxaYoTFNjkn4I0yAXgoIz4oMo/m8+Yvsdzn7UsZdTnM/mSRese06Z8Y9ebn5mdhgTesDFgjcC9xvlZnq7yt4kmYgNtpHbCDd4cOXVSfQR/ksmzbcUCFtWjRxggDd8z+b2xEgOXuaUdJhgwZldEGkHPemO2DA9hGyGHBfPYL3hix4CbAFj4sPuJrRWxve/nll4lXegEPCAiWLrbDYXTENXZGMPixrxNx0HhLlq5Si4m4dwresEyRN94B+MEPhgMjP8J27z3qts/brozYjYItiAAzrHaM8sgLMxFvz6AsVoC1s7yRjhd+4d9ftHgFwZJAPfQtUlZ5B7OCStmDd5Cxgzf6He+KwvZWMAX9AKzYtfuwAiz80+hXZiuad4NZraNBZ2CNIx2MMPRRsAicwiYLNhqDXbf8gnfo8QtKiLCaIRgc2IIHkLAgAA+4EBo0bGrEwRRI348Mn1SuXHmM5/joB7tB9KnS2nVb1fNJkz828mb3S+cuPY0wvBejL2+v43KYzwA44mAPqV72UaMnqv3l5vfZlRFKBX8d54UBakjIR4bCeXqGcgH6qVKldqlD+vQZ1AdDernN8WBFwCLhL0Xx7ldeeZX0nTbmNHp+ch28IAzGtrODN+oE4w3A5n6UNWt2lw0A2PWFGSe7ZVkO2CQBDnlafIRlDmMHaZE3wI0BAFY85xHsZ7/gzZXFfm18hGPlQ9YtP2yQ9yass+evGh/vYIGQv9rU36N/uIPwGzd/cHN9YMcJGovT2Z0xCFitZKNe5vfZlfH02SuEQc1qNPf0DHHNdcW9OQ+reKgbZIXBBQu4ZjeQpzR2MpHnAvZA64Av8OZ3wrjCLi2+189W3yn4qufgBbYLm/ubnn+wXocJ3t4qq8PbWzx5JrAQHYjcOuAPvEUX/NcFgbd8+GFp7Uhn8r8zicxcZSbwdpVHoPUj4PDGVB4ulUAXVPJzVhFEviLfQOuAwNtZnQo4vAOtAJKfswog8hX5OqUDAm9ndUvgLW4TmSWJDjiiAwJvgbcjiuWUtSH5OquwIt/gka/A29m2Mlvek/94/Bf9/OtjOUQGogOiA+HSAcBbWOIcS588eUZE1P4l/IhozZ9PntL/Hj2RQ2QgOiA6EC4dALyFJc6x9K+nCt7DGN4hdx88pNMXH8ghMhAdEB0Ilw4A3sIS51iKWQ0RNRR4S0eVjiY6EFAdEHg7B24MigJv6bAB7bBiaTnbYYNJvgJvZ3VB4C3wFniLDjiiAwJvgbcjihVMFoyU1dlOIPJ1Rr4Cb2fkyvoqlrdYXTI4ig44ogMCb4G3I4rFo5ecnVUwkW/Ula/A29m2F8tbrC4ZHEUHHNEBgXcEg/fchWuoQePWxvF+x140adpCOnn+nqEAx8/eoZZtOhlx9Pgr1uxW8cxxGjdtRz16D6VN248b+bRu19kyD85vUMh4I65YeM4qishX5OuvDvgC7xPn7lLIiMlUs3YjKlWmEn3QqTdt233G53796eebVNp6DVrSkZPfqnQ79p73yo3lq18wyN/6RLT4flveEBI+xkyZKg2lSp2OYsSMqe6LFitlAHznvvMqLHbsOJQiZWqXY9rMJUrAHCdOnLiUKlVaSpDwNZUmVqzYNGXGYhUnfYbMlChxUnUgHt776qvxjbBqNer53MgRTfBSHoFhZNcBO3gD3BUq1VT9OlmyFJQ7T351Da5s2XnKtm8fO32b0qfPpNKADWAKZLpq/T6KGy8exYnreiAOjpFjPrbNOxjaJszw3rrrtBLAgdCrVLBQUSWUuYvWqjAGc41aDTwKySrOkOGTVD5oELPwPuw6wOUd5udyLzAUHYhYOmAHbxhpgGnV6v8aYX36j1RhmLnbtWfnbgNV3EyZs6kzw9tTurz5Cqm/ZemPZe8pr4gQHm54oxK9+g5Xwhs+anq44I28kiVPqf5oqO6GQbjAO2J1zIigvFKGiK0TdvBmq3vTjhMGqI+d+U7NrtOmy2CEWbUzjEdY1hUr16IGjVrZwnvR0o0qDuJb5ReMYeGGNyBbsnRFJZjlq3aFC94YEeEeSZ0mvZuABd4Ru6MGo/JLmZ3VKTt4Z8mak5ImTe7W1/O/UVhZyN7aBxAGvAFxX+BdrkI1xShA3Fu+wfQszPB+u0RZKlOuKsEvjalP0xbvG0Jhl0iiREmUSwVuFRxt2ndxi5M1Wy7Comf9hi2VL/vll1+m8ZPnG/FYmAJvZzsay1nOIudA6YAdvMEHANz8vtJlKium7D/6jdszxMWmCTAHTMC9Hbw3bjtG0aNHpzx5C1jmZ35/sNyHGd7wHxV6szixv0lfaGR4Y7ESIx4f8FGxYDhOjBgx1AiKxsDRd8AoIw7HxVngLVDR9UGuI74+2MEblnP2HLnd+nvZ8lUVC/Ydvuz2DLvUAHysi2HBEnpgB+8mzdqr/EaOnemWXzDrUZjhzQuWqPz8xevVrhOsGOOewezPguXCz76kV+MnoHjxXjFWjXXBCrwjfmfV20uupb3s4A3jDjtLzLqCWX20aNGM3Wv6834DRxsgBtxxYJshDL81Xx6k0NO3XPKD9Y6dJ3DPwJ+u5xXs1wGBN4SQL39hJcA9hy6FCd7Ig1ea277XzU3IAm+BQbB3tqhWfjt458yVl2LGjGXsz2b5YLGSDUEO4zP7rnmmbj6bFyS79QpRXOrYua8bUzjPYD0HBN6Hjl1XIxv2fGN0C4vlDQEiH4ySiZMkcxslBd4C72DtZFG13Hbwbt6qgwLruEnzDLAuWbFNhdVt0MIIw55v9n8vXLKRsKVYPwoVLqbSdO8VQvqCJFwssO4xQOw6cNHIL7K0R5jh3bZ9V8Ko1u797pQ5S3YlPPiWIBiGd46cr1OX7oNcjpXr9rrEMbtWatdtqvIaO3GOi7AF3gLvyNLpoko97OCNr62x5hU/QULq1W8EDR46QX38h/Uz3rmGryFhXRctXsqFB7oMPfm8R42bpdLCraLHjyzXfsMbO0P0qQpGNcC7W88hhC+mIBi2oPV4fI0GQhyMhAirVbuxi2A/W7ZFhesb9xEf+SM+fOORRfhSDxmQIrMO2MEbdccX1/ham/mQMVNWmrNgtdHHV284oPzfWMT0JKtGTduq9GbrukjRkmqXSWT5HN5cf7/hbc4gPPf4vwjMH+MgP4RjyqPnferCfTp84oZLmP5crgWEogMRSwd8gTe3GWbrew99bdm/YQxyPKszGML/r4n+HC5c8wKm/jzYr/9f4R3swpPyRyxYSHtErPbwB97Sdv63ncBb/jtQr1aNdCr/O5XI7IXMBN7O6o7AW+At8BYdcEQHBN4Cb0cUS6wjZxVL5CvyFXg7qwNieYvVJYOj6IAjOiDwFng7olhiGTqrWCJfka/A21kdMFvew+99/5DOXvpeDpGB6IDoQLh0APAWljjH0l9+fUxE1OQl/Ihoy/Pnf9OzZ8/lEBmIDogOhEsHAG9hiXMsff7334D3GAVv+UckIBIQCYgERAIiAZGASEAkIBIQCYgERAIiAZGASEAkIBIQCYgERAIiAZGASEAkIBIQCYgERAIiAZGASEAkIBIQCYgERAIigcgsgf8DMArVc/zGJ4oAAAAASUVORK5CYII=)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GGOk9AHOu-5"
      },
      "source": [
        "Checking the fakeness score for scrapped data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "2Exw0w4HOQou",
        "outputId": "73018484-9605-4664-9a07-affceb6ddb05"
      },
      "source": [
        "import pandas as pd\n",
        "scrapped_data= pd.read_csv('https://raw.githubusercontent.com/anvithakp/AlternusVera/main/Politifact/politifact_new.tsv',sep='\\t', error_bad_lines=False)\n",
        "scrapped_data.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>meter</th>\n",
              "      <th>source</th>\n",
              "      <th>statement</th>\n",
              "      <th>context</th>\n",
              "      <th>tags</th>\n",
              "      <th>true</th>\n",
              "      <th>mostly-true</th>\n",
              "      <th>half-true</th>\n",
              "      <th>barely-true</th>\n",
              "      <th>false</th>\n",
              "      <th>pants-fire</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>half-true</td>\n",
              "      <td>Instagram posts</td>\n",
              "      <td>\"Kyle Rittenhouse used his COVID-19 stimulus c...</td>\n",
              "      <td>an Instagram post</td>\n",
              "      <td>['Criminal Justice', 'Guns', 'Wisconsin', 'Ins...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>31</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>false</td>\n",
              "      <td>Robin Vos</td>\n",
              "      <td>“We passed one of the first (COVID-19) bills i...</td>\n",
              "      <td>Interview</td>\n",
              "      <td>['Legal Issues', 'Wisconsin', 'Coronavirus', '...</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0      meter           source  ... barely-true false pants-fire\n",
              "0           0  half-true  Instagram posts  ...           9    31         12\n",
              "1           1      false        Robin Vos  ...           3     5          1\n",
              "\n",
              "[2 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGz3nfCtTkFc"
      },
      "source": [
        "scrapped_data['fakeness'] = \"True\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR0Q3mhWWKvk"
      },
      "source": [
        "##**Testing the Scrapped data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BT7cgrrP0Pl",
        "outputId": "1a1d67ea-4179-4f88-a3c0-a487e7767861"
      },
      "source": [
        "for i in range(len(scrapped_data)):\n",
        "  result = isFakeNews(scrapped_data['statement'][i],scrapped_data['source'][i],scrapped_data['context'][i],scrapped_data['mostly-true'][i],scrapped_data['half-true'][i],scrapped_data['barely-true'][i],scrapped_data['false'][i],scrapped_data['pants-fire'][i])\n",
        "  # print(fakeness)\n",
        "  if result >= 0.0 and result < 0.2 :\n",
        "      print(\"Pants on Fire\")\n",
        "      scrapped_data['fakeness'][i] = \"Pants on Fire\"\n",
        "  elif result >= 0.2 and result < 0.4 :\n",
        "      print(\"False\")\n",
        "      scrapped_data['fakeness'][i] = \"False\"\n",
        "  elif result >= 0.4 and result < 0.6 :\n",
        "      print(\"Barely True\")\n",
        "      scrapped_data['fakeness'][i] = \"Barely True\"\n",
        "  elif result >= 0.4 and result < 0.6 :\n",
        "      print(\"Half True\")\n",
        "      scrapped_data['fakeness'][i] = \"Half True\"\n",
        "  elif result >= 0.6 and result < 0.8 :\n",
        "      print(\"Mostly True\")\n",
        "      scrapped_data['fakeness'][i] = \"Mostly True\"\n",
        "  else:\n",
        "      print(\"True\")\n",
        "      scrapped_data['fakeness'][i] = \"True\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.22460622987838091\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.32045996913237235\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.23427030518658248\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.19870075133683815\n",
            "Pants on Fire\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.2862010065479975\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.34854998927139463\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.3543100895671337\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.2569238801782656\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.41229667261066494\n",
            "Barely True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.2786198649703319\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.4368905381704158\n",
            "Barely True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.2415984038184556\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.4201343672193753\n",
            "Barely True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.2658365154657736\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.2132231161836198\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.2271347199617284\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.2745774656340586\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.3550191556809375\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.4927584235709608\n",
            "Barely True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.29314176974441153\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.26070478789397933\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.16721782226261317\n",
            "Pants on Fire\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.3679594678932493\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.45621662476012215\n",
            "Barely True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.42390184720510066\n",
            "Barely True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.5019940871109121\n",
            "Barely True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.29261149891181504\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.3638742947439891\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.27000345863685354\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "probTotal 0.2655303181031695\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/seekers_bertmcc.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
            "/content/seekers_bertmcc.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
            "/content/seekers_bertmcc.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  tensorProbability = F.softmax(torch.tensor(predictions))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqmYO0lEVLeb"
      },
      "source": [
        "##**Comparing the predicted results with actual values for the scrapped data**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N32MZL1V87i"
      },
      "source": [
        "truth-o-meter = predicted values   \n",
        "\n",
        "meter = actual values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "C9K-4FSJUfFD",
        "outputId": "f9a7b23a-15b3-4d86-edc5-7712afb05c4b"
      },
      "source": [
        "scrapped_data['truth-o-meter'] = scrapped_data['fakeness']\n",
        "col = ['statement','truth-o-meter','meter']\n",
        "result_data = scrapped_data[col]\n",
        "result_data.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>statement</th>\n",
              "      <th>truth-o-meter</th>\n",
              "      <th>meter</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"Kyle Rittenhouse used his COVID-19 stimulus c...</td>\n",
              "      <td>False</td>\n",
              "      <td>half-true</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>“We passed one of the first (COVID-19) bills i...</td>\n",
              "      <td>False</td>\n",
              "      <td>false</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Video shows movers carrying boxes out of the W...</td>\n",
              "      <td>False</td>\n",
              "      <td>false</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Dominion and Smartmatic “have closed up shop” ...</td>\n",
              "      <td>Pants on Fire</td>\n",
              "      <td>false</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>“Pennsylvania just banned alcohol sales.”</td>\n",
              "      <td>False</td>\n",
              "      <td>mostly-false</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Photo shows “Biden’s inaugural stage being bui...</td>\n",
              "      <td>False</td>\n",
              "      <td>false</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Says Donald Trump’s sister tweeted, “My brothe...</td>\n",
              "      <td>False</td>\n",
              "      <td>false</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Democrats “spent 4 years refusing to acknowled...</td>\n",
              "      <td>False</td>\n",
              "      <td>false</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>North Carolina Gov. Roy Cooper’s “daughter is ...</td>\n",
              "      <td>Barely True</td>\n",
              "      <td>false</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>“In certain swing states, there were more vote...</td>\n",
              "      <td>False</td>\n",
              "      <td>fire</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           statement  ...         meter\n",
              "0  \"Kyle Rittenhouse used his COVID-19 stimulus c...  ...     half-true\n",
              "1  “We passed one of the first (COVID-19) bills i...  ...         false\n",
              "2  Video shows movers carrying boxes out of the W...  ...         false\n",
              "3  Dominion and Smartmatic “have closed up shop” ...  ...         false\n",
              "4          “Pennsylvania just banned alcohol sales.”  ...  mostly-false\n",
              "5  Photo shows “Biden’s inaugural stage being bui...  ...         false\n",
              "6  Says Donald Trump’s sister tweeted, “My brothe...  ...         false\n",
              "7  Democrats “spent 4 years refusing to acknowled...  ...         false\n",
              "8  North Carolina Gov. Roy Cooper’s “daughter is ...  ...         false\n",
              "9  “In certain swing states, there were more vote...  ...          fire\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}