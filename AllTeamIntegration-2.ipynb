{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AllTeamIntegration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvPovJRDX7KW"
      },
      "source": [
        "#Integrating Blastoff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQedchzvX9D_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ea977187-c91f-4726-fb12-39182884eea7"
      },
      "source": [
        "#Content Statistics\n",
        "%cd /content/\n",
        "!git clone https://github.com/Wayne122/Content_Statistics_Demo.git\n",
        "%cd /content/Content_Statistics_Demo\n",
        "!pip install -r requirements.txt\n",
        "!python3 -m spacy download en_core_web_lg\n",
        "\n",
        "#Sensationalism\n",
        "%cd /content/\n",
        "!git clone https://github.com/roanacla/nlp_sensationalism.git\n",
        "!pip install -r /content/nlp_sensationalism/requirements.txt\n",
        "\n",
        "#Context Veracity\n",
        "%cd /content/\n",
        "!git clone https://github.com/snarvekark/Veracity_Factor.git\n",
        "%cd /content/Veracity_Factor/\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'Content_Statistics_Demo'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 31 (delta 15), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (31/31), done.\n",
            "/content/Content_Statistics_Demo\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (1.1.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (3.2.5)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (2.2.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (0.17.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 3)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 3)) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->-r requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 5)) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 5)) (2.0.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 5)) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 5)) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 5)) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 5)) (1.0.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 5)) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 5)) (50.3.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 5)) (1.0.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 5)) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 5)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 5)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 5)) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 5)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 5)) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->-r requirements.txt (line 5)) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->-r requirements.txt (line 5)) (3.4.0)\n",
            "Collecting en_core_web_lg==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9MB 107.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.4.0)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-cp36-none-any.whl size=829180944 sha256=74ce263d2c2147975d06bcaf8cbdb4a3f7c658563cf7d77a4987de2a5346d43b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ngj17e1z/wheels/2a/c1/a6/fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n",
            "/content\n",
            "Cloning into 'nlp_sensationalism'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 19 (delta 7), reused 16 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (19/19), done.\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (from -r /content/nlp_sensationalism/requirements.txt (line 1)) (4.0.0)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from -r /content/nlp_sensationalism/requirements.txt (line 2)) (0.4)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (0.9.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (3.0.12)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (2020.11.8)\n",
            "/content\n",
            "Cloning into 'Veracity_Factor'...\n",
            "remote: Enumerating objects: 51, done.\u001b[K\n",
            "remote: Counting objects: 100% (51/51), done.\u001b[K\n",
            "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
            "remote: Total 51 (delta 21), reused 36 (delta 9), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (51/51), done.\n",
            "/content/Veracity_Factor\n",
            "Collecting git+https://github.com/abenassi/Google-Search-API (from -r requirements.txt (line 11))\n",
            "  Cloning https://github.com/abenassi/Google-Search-API to /tmp/pip-req-build-e0tsaap9\n",
            "  Running command git clone -q https://github.com/abenassi/Google-Search-API /tmp/pip-req-build-e0tsaap9\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (3.2.5)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (4.6.3)\n",
            "Collecting selenium<3.0.0,>=2.44.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/f8/e46684764161fee7b7d2e0fd9b82d56915438b68e498181dc45183643c82/selenium-2.53.6-py2.py3-none-any.whl (884kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (2.23.0)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 19.8MB/s \n",
            "\u001b[?25hCollecting vcrpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/62/571e9fa5c2a2c986c001d1be99403a5e800d2e72b905e6b1e951148c75c9/vcrpy-4.1.1-py2.py3-none-any.whl (40kB)\n",
            "\u001b[K     |████████████████████████████████| 40kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (0.16.0)\n",
            "Collecting fake-useragent\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
            "Collecting newspaper3k\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 22.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 4)) (2020.11.8)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from vcrpy->-r requirements.txt (line 6)) (3.13)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from vcrpy->-r requirements.txt (line 6)) (1.12.1)\n",
            "Collecting yarl; python_version >= \"3.6\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/08/52b26b44bce7b818b410aee37c5e424c9ea420c557bca97dc2adac29b151/yarl-1.6.3-cp36-cp36m-manylinux2014_x86_64.whl (293kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 19.6MB/s \n",
            "\u001b[?25hCollecting jieba3k>=0.35.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 22.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k->-r requirements.txt (line 9)) (2.8.1)\n",
            "Collecting tldextract>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/62/b6acd3129c5615b9860e670df07fd55b76175b63e6b7f68282c7cad38e9e/tldextract-3.1.0-py2.py3-none-any.whl (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k->-r requirements.txt (line 9)) (7.0.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k->-r requirements.txt (line 9)) (4.2.6)\n",
            "Collecting tinysegmenter==0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
            "Collecting feedfinder2>=0.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
            "Collecting feedparser>=5.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/21/faf1bac028662cc8adb2b5ef7a6f3999a765baa2835331df365289b0ca56/feedparser-6.0.2-py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.5MB/s \n",
            "\u001b[?25hCollecting cssselect>=0.9.2\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->-r requirements.txt (line 10)) (0.22.2.post1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from yarl; python_version >= \"3.6\"->vcrpy->-r requirements.txt (line 6)) (3.7.4.3)\n",
            "Collecting multidict>=4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/95/f89c97db4d27b24fce7c6fb6ef05228f347e43bbf67e4737a0660c8753fd/multidict-5.0.2-cp36-cp36m-manylinux2014_x86_64.whl (141kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 44.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k->-r requirements.txt (line 9)) (3.0.12)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
            "Collecting sgmllib3k\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 10)) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 10)) (0.17.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 10)) (1.4.1)\n",
            "Building wheels for collected packages: fake-useragent, Google-Search-API, jieba3k, tinysegmenter, feedfinder2, sgmllib3k\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-cp36-none-any.whl size=13485 sha256=12ed5cd1b9a39fc59adb32f61dd93112b8fe13dd1561c7c23b182c660e9062df\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/63/09/d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
            "  Building wheel for Google-Search-API (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Google-Search-API: filename=Google_Search_API-1.1.14-cp36-none-any.whl size=607835 sha256=59e25fc9d3932a7889aecfad14eb9bd13336ccea91de6c4ac9de245bc0513521\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jlga3nen/wheels/9d/dd/52/f744758aa94909328835af5d6f1650c9c3d8cf2e6e8988767a\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp36-none-any.whl size=7398406 sha256=68ee1a13698467e334aeaa7aa3e133a5e6af1ea390ef62159a49183507f620de\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp36-none-any.whl size=13538 sha256=a3bc65f315f8c7f075736b1f16cb2ef1bd20d488442c0f7147c955be1db5482d\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp36-none-any.whl size=3355 sha256=ed895f43d3a50852df88cdea4e9fc7e3a03ba5c3f8a0f7a791b66229f3856892\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp36-none-any.whl size=6067 sha256=bd15d1d9d281ba71322db20fc3031dc547fb75efd2cdadfad290f68a5f158ae2\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
            "Successfully built fake-useragent Google-Search-API jieba3k tinysegmenter feedfinder2 sgmllib3k\n",
            "Installing collected packages: selenium, unidecode, multidict, yarl, vcrpy, fake-useragent, jieba3k, requests-file, tldextract, tinysegmenter, feedfinder2, sgmllib3k, feedparser, cssselect, newspaper3k, Google-Search-API\n",
            "Successfully installed Google-Search-API-1.1.14 cssselect-1.1.0 fake-useragent-0.1.11 feedfinder2-0.0.4 feedparser-6.0.2 jieba3k-0.35.1 multidict-5.0.2 newspaper3k-0.2.8 requests-file-1.5.1 selenium-2.53.6 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.1.0 unidecode-1.1.1 vcrpy-4.1.1 yarl-1.6.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "jieba"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQGV2C3XipND"
      },
      "source": [
        "# ***IMPORTANT MESSAGE***\n",
        "Because this is important, we're going to say it three times.\n",
        "\n",
        "**YOU HAVE TO RESTART THE RUNTIME AFTER RUNNING THE ABOVE CELL**\n",
        "\n",
        "**YOU HAVE TO RESTART THE RUNTIME AFTER RUNNING THE ABOVE CELL**\n",
        "\n",
        "**YOU HAVE TO RESTART THE RUNTIME AFTER RUNNING THE ABOVE CELL**\n",
        "\n",
        "Whoever touched our code ignored this message. Please do not ignore it again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVWHRFNPZlsV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0bfa6c0-4bcc-4d04-db29-cd36f3572c44"
      },
      "source": [
        "#Content Statistics\n",
        "%cd /content/Content_Statistics_Demo/\n",
        "from Blastoff_Content_Statistics import BlastoffContentStatistics\n",
        "bcs = BlastoffContentStatistics()\n",
        "\n",
        "#Sensationalims\n",
        "%cd /content/\n",
        "from nlp_sensationalism.sensaScorer import SensaScorer\n",
        "sensa = SensaScorer()\n",
        "\n",
        "#Context Veracity\n",
        "%cd /content/Veracity_Factor/\n",
        "from context_veracity import Context_Veracity\n",
        "cv = Context_Veracity()\n",
        "\n",
        "%cd /content/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Content_Statistics_Demo\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "spaCy not installed, install with \"python3 -m spacy download en_core_web_lg\" then restart the runtime.\n",
            "Downloading 1oTfsNgkmEBemkVfrWSjnc-K9svmL7Iak into ./bcs_encoder.zip... Done.\n",
            "Loading default pretrained model.\n",
            "Downloading 1oFjoL9LWrp2-YPSJL2UhBQ1efV9LiC2n into ./content_statistic_model.pickle... Done.\n",
            "/content/Veracity_Factor\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Downloading 18NyBuNHikHiUzrAC4oOMUOO5KAaKouEY into ./context_veracity_models.zip... Done.\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJOsPwW2An_A"
      },
      "source": [
        "#GirlsWhoCode Team Integration\n",
        " \n",
        "\n",
        "* Prarthana - Toxicity\n",
        "* Manasi - Topics\n",
        "* Jyothi - Political Affiliation\n",
        "* Akanksha - Biases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t4Yx3_rGhVy"
      },
      "source": [
        "#Needed for cereal killers\n",
        "#! pip install autogluon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc-lLqiwxZhl",
        "outputId": "7f3148de-463d-462c-d99e-3e1950629e2c"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jJ2HLXgYeox",
        "outputId": "02d126b3-2c13-4c8a-ff88-6d83ef07bfd8"
      },
      "source": [
        "!pip install ktrain"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ktrain\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/52/aa566f14064dd8904e37d2346431465f20a053df8ad89eae246dcc073dc5/ktrain-0.25.0.tar.gz (25.3MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3MB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.22.2.post1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from ktrain) (3.2.2)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from ktrain) (1.1.4)\n",
            "Requirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.6/dist-packages (from ktrain) (1.0.0)\n",
            "Collecting keras_bert>=0.86.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e2/7f/95fabd29f4502924fa3f09ff6538c5a7d290dfef2c2fe076d3d1a16e08f0/keras-bert-0.86.0.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.23.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.17.0)\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 38.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.42.1)\n",
            "Collecting cchardet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/e5/a0b9edd8664ea3b0d3270c451ebbf86655ed9fc4c3e4c45b9afae9c2e382/cchardet-2.1.7-cp36-cp36m-manylinux2010_x86_64.whl (263kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 44.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.5)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.1.1)\n",
            "Collecting seqeval==0.0.19\n",
            "  Downloading https://files.pythonhosted.org/packages/93/e5/b7705156a77f742cfe4fc6f22d0c71591edb2d243328dff2f8fc0f933ab6/seqeval-0.0.19.tar.gz\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from ktrain) (20.4)\n",
            "Collecting transformers>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/84/7bc03215279f603125d844bf81c3fb3f2d50fe8e511546eb4897e4be2067/transformers-4.0.0-py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 42.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from ktrain) (5.5.0)\n",
            "Collecting syntok\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/76/a49e73a04b3e3a14ce232e8e28a1587f8108baa665644fe8c40e307e792e/syntok-1.3.1.tar.gz\n",
            "Collecting whoosh\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/19/24d0f1f454a2c1eb689ca28d2f178db81e5024f42d82729a4ff6771155cf/Whoosh-2.7.4-py2.py3-none-any.whl (468kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 42.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->ktrain) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->ktrain) (1.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.1->ktrain) (2018.9)\n",
            "Requirement already satisfied: Keras>=2.4.3 in /usr/local/lib/python3.6/dist-packages (from keras_bert>=0.86.0->ktrain) (2.4.3)\n",
            "Collecting keras-transformer>=0.38.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/6c/d6f0c164f4cc16fbc0d0fea85f5526e87a7d2df7b077809e422a7e626150/keras-transformer-0.38.0.tar.gz\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect->ktrain) (1.15.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.3->ktrain) (4.4.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (3.7.4.3)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (5.1.1)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (3.13)\n",
            "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (2.11.2)\n",
            "Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (7.0.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 41.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.1.0->ktrain) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=3.1.0->ktrain) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.1.0->ktrain) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.1.0->ktrain) (2019.12.20)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 48.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (4.3.3)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (50.3.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (0.7.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras_bert>=0.86.0->ktrain) (2.10.0)\n",
            "Collecting keras-pos-embd>=0.11.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.27.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/32/45adf2549450aca7867deccfa04af80a0ab1ca139af44b16bc669e0e09cd/keras-multi-head-0.27.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/57/ef/61a1e39082c9e1834a2d09261d4a0b69f7c818b359216d4e1912b20b1c86/keras-embed-sim-0.8.0.tar.gz\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh->ktrain) (1.1.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.1.0->ktrain) (7.1.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->ktrain) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->ktrain) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ktrain) (0.2.5)\n",
            "Collecting keras-self-attention==0.46.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/6b/c804924a056955fa1f3ff767945187103cfc851ba9bd0fc5a6c6bc18e2eb/keras-self-attention-0.46.0.tar.gz\n",
            "Building wheels for collected packages: ktrain, keras-bert, langdetect, seqeval, syntok, keras-transformer, sacremoses, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ktrain: filename=ktrain-0.25.0-cp36-none-any.whl size=25274500 sha256=7f929657d1a13cfe09d566c78769fec71f0a2275132744aa1b9f8fd1d84165d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/73/05/f36d0027bb6575384e21506dbba8db36a7825f15a24f09b2d5\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.86.0-cp36-none-any.whl size=34145 sha256=4ffcb731e56610c087e717344f148c81be381b3bdf2560d8b32f9c1772f95068\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/f0/b1/748128b58562fc9e31b907bb5e2ab6a35eb37695e83911236b\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993195 sha256=c898e705bd9d772b9f1f3bf5600fc542ace1bbff8703648dcc96cd37e1e4c7e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.19-cp36-none-any.whl size=9919 sha256=fdd7cf0222f9a9654a55e0f90b708e30d9c70d810a63b2f67e8603b655118c1b\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/1f/bf/1198beceed805a2099060975f6281d1b01046dd279e19c97be\n",
            "  Building wheel for syntok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for syntok: filename=syntok-1.3.1-cp36-none-any.whl size=20919 sha256=9b34e7da175614bdb496f0d3067c31fa5697a1537e1d7a0baf74550ce2b138d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/c6/a4/be1920586c49469846bcd2888200bdecfe109ec421dab9be2d\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.38.0-cp36-none-any.whl size=12942 sha256=7adc6e197787bd422e6e1877d2b6a8c268382ef45d478ea989bbbb517de692f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/fb/3a/37b2b9326c799aa010ae46a04ddb04f320d8c77c0b7e837f4e\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=0de54fa5b7198aeb32ce39bdc1a488a873e4f248e2a15fdc0a8e398fb98e0e08\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7554 sha256=a712af6bf0bff22969afe4467b8a274f50d578b73817263139138ba62838be5b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-cp36-none-any.whl size=15612 sha256=ca7ae62f8c514a38f653a412392bb5e726bf37f098496b87e0b72491d397cf56\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b4/49/0a0c27dcb93c13af02fea254ff51d1a43a924dd4e5b7a7164d\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=943d82ac241e82812b28cb51106258e0155a63c1d054b4142c56b974fe1553cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5626 sha256=9e795da0cb63932f291fcdd65a37ab1a4387247cfb564d6e2575049e0260e321\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.8.0-cp36-none-any.whl size=4559 sha256=4b50737363365f595d9980fafb79552d218282c90a1ed1e983f2d0cceaa7605b\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/45/8b/c111f6cc8bec253e984677de73a6f4f5d2f1649f42aac191c8\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-cp36-none-any.whl size=17278 sha256=91c6c25772c0ecc1e5ea6a9d97ff38b56993cbf48bd0f5db3338e7022f7edba3\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/2e/80/fec4c05eb23c8e13b790e26d207d6e0ffe8013fad8c6bdd4d2\n",
            "Successfully built ktrain keras-bert langdetect seqeval syntok keras-transformer sacremoses keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert, langdetect, cchardet, seqeval, sacremoses, tokenizers, transformers, syntok, whoosh, ktrain\n",
            "Successfully installed cchardet-2.1.7 keras-bert-0.86.0 keras-embed-sim-0.8.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.38.0 ktrain-0.25.0 langdetect-1.0.8 sacremoses-0.0.43 seqeval-0.0.19 syntok-1.3.1 tokenizers-0.9.4 transformers-4.0.0 whoosh-2.7.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCr2tCSGYvvi",
        "outputId": "1a40b543-f75f-48ca-8190-b01c0eb54ff6"
      },
      "source": [
        "!pip install vaderSentiment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vaderSentiment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/fc/310e16254683c1ed35eeb97386986d6c00bc29df17ce280aed64d55537e9/vaderSentiment-3.3.2-py2.py3-none-any.whl (125kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 23.2MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 22.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30kB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 112kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNMLNIyLot5l",
        "outputId": "24bb6460-3c90-4e19-9c0b-bca50543c720"
      },
      "source": [
        "! pip install textstat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting textstat\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/b1/ab40a00b727a0d209402d1be6aa3f1bc75bd03678b59ace8507b08bf12f5/textstat-0.7.0-py3-none-any.whl (99kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 4.8MB/s \n",
            "\u001b[?25hCollecting pyphen\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/5a/5bc036e01389bc6a6667a932bac3e388de6e7fa5777a6ff50e652f60ec79/Pyphen-0.10.0-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 10.6MB/s \n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.10.0 textstat-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31W360VW9I2l"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVXabOgYqQa2",
        "outputId": "ef34537f-10b5-425c-a129-9c14c4174bd7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSup7Y_Ysm6X"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVyNTIqZwgxT"
      },
      "source": [
        "##GirlsWhoCode_Toxicity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcwGfkUUAkqZ"
      },
      "source": [
        "toxicity = drive.CreateFile({'id':'1Hj3yQPC1j02HMF2Lr1P4VOlZ1B1VX-sz'})\n",
        "toxicity.GetContentFile('toxicityclass1.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_hS-drVwjoy"
      },
      "source": [
        "##GirlsWhoCode_Topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xezsg7IPSN6s"
      },
      "source": [
        "topics_bigram_LDA = drive.CreateFile({'id':'1UfC_gZu29xzbnx7_dZ2gzEKTXqq7sRkC'})\n",
        "topics_bigram_LDA.GetContentFile('Topics_with_LDA_Bigram.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz-oArZawpXg"
      },
      "source": [
        "##GirlsWhoCode_Bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybFzAN_xcQij"
      },
      "source": [
        "bias = drive.CreateFile({'id':'1tmFa3ZeBJUHoKAq-qaXxhgZZ-2D4MP5Z'})\n",
        "bias.GetContentFile('bias.py')\n",
        "#bias = drive.CreateFile({'id': '17chwyVx76T6bKNVwqU5YRU41mcp0K_ZZ'})\n",
        "#bias.GetContentFile('girlswhocode_bias.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-IGyMmlwvRh"
      },
      "source": [
        "##GirlsWhoCode_PoliticalAffiliation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qu6BtNw4eQMj"
      },
      "source": [
        "political_affiliation = drive.CreateFile({'id':'1npn7-zVny2H_-uuVsq1eKKl7vHSuL9Bj'})\n",
        "political_affiliation.GetContentFile('girlswhocode_political_affiliation.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLVw8GlHdyin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb37ff6-0d98-4b59-b45f-faeb8c241c9a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "import math\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XMo6gRTYH5j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b350bc6-857b-44ad-f05f-0a9afad63c54"
      },
      "source": [
        "from toxicityclass1 import GirlsWhoCode_Toxicity\n",
        "from Topics_with_LDA_Bigram import Topics_with_LDA_Bigram\n",
        "#from girlswhocode_bias import Gwc_Bias\n",
        "from bias import Gwc_Bias\n",
        "from girlswhocode_political_affiliation import Girlswhocode_PoliticalAfiiliation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLDRYB_3xrYP"
      },
      "source": [
        "#Integrating Team_seekers\n",
        "\n",
        "* Alekhya - Spam\n",
        "* Anvitha - Bert\n",
        "* Jahnavi - Stance Detection\n",
        "* Manisha - ClickBait "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iejtqxjXxqZ8",
        "outputId": "0d9b9460-6ade-42e2-b4dd-dff0f7fa3872"
      },
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL_MY00ExwjV",
        "outputId": "5f7cf95d-4bf5-48c8-8908-820abb31e286"
      },
      "source": [
        "!wget -O bertmodel https://www.dropbox.com/sh/r7hzurrs6a2x461/AABySI0lXYNT0OCwuXre_Qh1a?dl=0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-02 02:48:45--  https://www.dropbox.com/sh/r7hzurrs6a2x461/AABySI0lXYNT0OCwuXre_Qh1a?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.1, 2620:100:6018:1::a27d:301\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /sh/raw/r7hzurrs6a2x461/AABySI0lXYNT0OCwuXre_Qh1a [following]\n",
            "--2020-12-02 02:48:45--  https://www.dropbox.com/sh/raw/r7hzurrs6a2x461/AABySI0lXYNT0OCwuXre_Qh1a\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucf873e2669ae055b6e9b8b3fef6.dl.dropboxusercontent.com/zip_download_get/AnqsJri0WFHH8-R2GUaq0EHtUUSUynOW1R_wPP_Wy938XeCApYSRVTkqrql-9yryWciL-6to1cIsQLZ-5_THAASKswAdLwe7YTJ7kccYEol0_A [following]\n",
            "--2020-12-02 02:48:46--  https://ucf873e2669ae055b6e9b8b3fef6.dl.dropboxusercontent.com/zip_download_get/AnqsJri0WFHH8-R2GUaq0EHtUUSUynOW1R_wPP_Wy938XeCApYSRVTkqrql-9yryWciL-6to1cIsQLZ-5_THAASKswAdLwe7YTJ7kccYEol0_A\n",
            "Resolving ucf873e2669ae055b6e9b8b3fef6.dl.dropboxusercontent.com (ucf873e2669ae055b6e9b8b3fef6.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to ucf873e2669ae055b6e9b8b3fef6.dl.dropboxusercontent.com (ucf873e2669ae055b6e9b8b3fef6.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 438268143 (418M) [application/zip]\n",
            "Saving to: ‘bertmodel’\n",
            "\n",
            "bertmodel           100%[===================>] 417.96M  54.2MB/s    in 7.7s    \n",
            "\n",
            "2020-12-02 02:48:54 (54.0 MB/s) - ‘bertmodel’ saved [438268143/438268143]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CY-aycJ3x4dE",
        "outputId": "f8a8d601-d2c3-44f8-966f-9281740bef5e"
      },
      "source": [
        "!unzip bertmodel"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  bertmodel\n",
            "warning:  stripped absolute path spec from /\n",
            "mapname:  conversion of  failed\n",
            " extracting: vocab.txt               \n",
            " extracting: config.json             \n",
            " extracting: pytorch_model.bin       \n",
            " extracting: tokenizer_config.json   \n",
            " extracting: special_tokens_map.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1gNuJALx46m"
      },
      "source": [
        "#https://drive.google.com/file/d/1CDJbp6anxmxvG1tS1cyCcvCtrmr3z1fM/view?usp=sharing\n",
        "bertmcc = drive.CreateFile({'id': '1CDJbp6anxmxvG1tS1cyCcvCtrmr3z1fM'})\n",
        "bertmcc.GetContentFile('seekers_bertmcc.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7w9_6aRx7gh",
        "outputId": "65d20b33-b586-4ef4-afcb-c42459312782"
      },
      "source": [
        "from seekers_bertmcc import Seekers_BertMcc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIDTAJG1x-M-",
        "outputId": "6adc3737-e31d-497f-c582-63b17dd013da"
      },
      "source": [
        "#https://drive.google.com/file/d/1Cb0wjMxLsIaalKwqGLmAaimD0nTERWmJ/view?usp=sharing\n",
        "stance = drive.CreateFile({'id': '1Cb0wjMxLsIaalKwqGLmAaimD0nTERWmJ'})\n",
        "stance.GetContentFile('seekers_stancedetection.py')\n",
        "from seekers_stancedetection import Seekers_StanceDetection"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3WrA_alyBVW",
        "outputId": "a98ee3a2-21ff-4ae7-ad35-6eea608a63df"
      },
      "source": [
        "#https://drive.google.com/file/d/1RC8LZBnTR3-kmoxTP_usBcRwTTEqSTZL/view?usp=sharing\n",
        "clickbait = drive.CreateFile({'id': '1RC8LZBnTR3-kmoxTP_usBcRwTTEqSTZL'})\n",
        "clickbait.GetContentFile('seekers_clickbait.py')\n",
        "from seekers_clickbait import Seekers_ClickBait"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEj5tiWOyLlW",
        "outputId": "eae4c9e8-d5cf-4632-ecff-e8c96e3465a6"
      },
      "source": [
        "spam = drive.CreateFile({'id':'1Ku30Kezz0NUzfBlQll8tKVJaFUQAKUpO'}) # replace the id with id of file you want to access\n",
        "spam.GetContentFile('seekers_spam.py')\n",
        "from seekers_spam import Seekers_Spam"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Svbn8nkhNNTl"
      },
      "source": [
        "#Integrating TrailBlazers\n",
        "* Jignesh - Misleading Intentions\n",
        "* Pankaj - Education \n",
        "* Manish - Event Coverage\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTZsTTN4NRQ9",
        "outputId": "f1f9a4da-c5d6-4adc-c4a8-294cfa333fd6"
      },
      "source": [
        "!git clone https://github.com/pankajhpatil/AlternusVera_Education.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'AlternusVera_Education'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 17 (delta 8), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (17/17), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSsmr-q2OdLW",
        "outputId": "1658092e-70a0-459f-d053-fae51dd48d9c"
      },
      "source": [
        "!git clone https://github.com/Manish0112/AlternusVera_EventDetection.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'AlternusVera_EventDetection'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 12 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (12/12), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubIObwcBELZ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3f8cac5-81c9-498e-a546-d3352b04af79"
      },
      "source": [
        "!git clone https://github.com/jignesh-sjsu/AlternusVera_MisleadingIntention.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'AlternusVera_MisleadingIntention'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 60 (delta 22), reused 18 (delta 7), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (60/60), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV2_xqbiOgW-"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('/content/AlternusVera_Education')\n",
        "sys.path.append('/content/AlternusVera_EventDetection')\n",
        "sys.path.append('/content/AlternusVera_MisleadingIntention')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtpURc7mOn31",
        "outputId": "f8a50298-15fb-4d58-a3b1-530ff34de138"
      },
      "source": [
        "import alternusvera_education as AVE\n",
        "import EventCoverage as EC\n",
        "import alternusvera_misleading_intentions as MI"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx8pgjECa0eC"
      },
      "source": [
        "Label_map={'barely-true': 0,\n",
        "           'false' : 1,\n",
        "           'full-flop' : 2,\n",
        "           'half-flip' : 3,\n",
        "           'half-true' : 4,\n",
        "           'mostly-true' : 5,\n",
        "           'pants-fire' : 6,\n",
        "           'true' : 7}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnTdG0OEAllP"
      },
      "source": [
        "#Integrating Go ML\n",
        "\n",
        "* Amit Vijapure - Stance Detection\n",
        "* Dhwani Sanghvi - Writing Style\n",
        "* Mayuri Bhise - Clickbait\n",
        "* Praveen Kumar Thakur - News Coverage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMjQr9UBA1VJ"
      },
      "source": [
        "go_ml_factors = drive.CreateFile({'id': 'https://drive.google.com/file/d/1fLdzrlv0WjzIJtkUUMGFJ50s6pp93pkJ/view?usp=sharing'.split('/')[-2]})\n",
        "go_ml_factors.GetContentFile('go_ml.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKMc6QYxCS2s",
        "outputId": "922d3f84-e836-4152-9ae9-d3274e4ce540"
      },
      "source": [
        "from go_ml import *"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "0.8\n",
            "0.48774299521917436\n",
            "Half-True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnn4uNLXFITQ"
      },
      "source": [
        "#Integrating Sigma\n",
        "\n",
        "* Sergio Aguilar (Malicious Account)\n",
        "* Achalaesh Lanka (Network based)\n",
        "* Indrayani Bhalerao (Credibility)\n",
        "* Geethu Padachery (Verifiable Authenticity)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYSu_rn-F3Zj",
        "outputId": "e6eb8a44-0226-4bf8-e070-14a0d43d0475"
      },
      "source": [
        "!rm -rf /content/cmpe257_AlternusVera_SIGMA\n",
        "!apt-get install protobuf-compiler\n",
        "!git clone https://github.com/sacefe/cmpe257_AlternusVera_SIGMA.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "protobuf-compiler is already the newest version (3.0.0-9.1ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Cloning into 'cmpe257_AlternusVera_SIGMA'...\n",
            "remote: Enumerating objects: 39, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 39 (delta 16), reused 31 (delta 11), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (39/39), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nW0_oGhQJjXb",
        "outputId": "bce46005-7b70-4325-d0d2-9d64e19b1a2a"
      },
      "source": [
        "#run only one time\n",
        "# if False:\n",
        "!python -m spacy download en_core_web_md\n",
        "import spacy\n",
        "from spacy.matcher import Matcher \n",
        "from spacy.tokens import Span \n",
        "import en_core_web_md\n",
        "\n",
        "nlp = en_core_web_md.load()\n",
        "Loaded_en_core = True"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4MB 73.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.4.0)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-cp36-none-any.whl size=98051305 sha256=1d150d6f7f4b5821f5f256cc83464dc1b829398ca9b41d2460bf53322ff66c21\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-t22k_pib/wheels/df/94/ad/f5cf59224cea6b5686ac4fd1ad19c8a07bc026e13c36502d81\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IInzs0-0JmJv"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('/content/cmpe257_AlternusVera_SIGMA')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAYnMV2QJqwU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71ec3be1-6f7e-4808-d87d-7181b1226d7f"
      },
      "source": [
        "from SIGMAmaliciousAccount import MalicousAccount\n",
        "from SIGMAcredibility  import Credibility\n",
        "from SIGMA_networkbased  import NetworkBasedPredictor\n",
        "from SIGMAauthenticity  import VerifiableAuthenticity\n",
        "\n",
        "#Create Malicous object\n",
        "maObject = MalicousAccount() \n",
        "malicious_accountCls = maObject.load(model2load=\"/content/cmpe257_AlternusVera_SIGMA/malicious_account_MLP.pkl\")\n",
        "#Create Credibility object\n",
        "credObject = Credibility() \n",
        "credCls = credObject.load(model2load=\"/content/cmpe257_AlternusVera_SIGMA/credibility_model.pkl\")\n",
        "#Create Network Based object\n",
        "nb = NetworkBasedPredictor()\n",
        "nb.load('/content/cmpe257_AlternusVera_SIGMA/networkbased.pkl')\n",
        "#Create Verifiable Authenticity object\n",
        "va = VerifiableAuthenticity()\n",
        "\n",
        "# TextToPredict  = 'sdfgsdfgsdfg sdfgsdfg xcghxdh xcghgh'\n",
        "# venue = 'CNN'\n",
        "\n",
        "# #Predict \n",
        "# predMalAcc, factorMalAcc, labelMalAcc  = maObject.predict(malicious_accountCls, TextToPredict, nlp)\n",
        "# credLabel, factorCred = credObject.predict(credCls, TextToPredict , nlp)\n",
        "# factorNB= nb.predict(TextToPredict, nlp)\n",
        "# factorVA = va.predict(TextToPredict, venue)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etYBi9bWP6JB"
      },
      "source": [
        "#Integrating Feature Finders\n",
        "\n",
        "*   Juhi Nayak (Stance Detection)\n",
        "*   Sneha Patil (Reliable Source)\n",
        "*   Rashmi Sarode (Toxicity)\n",
        "*   Chi Hoang (Title vs Body)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8BLbgCoP857",
        "outputId": "bd052b48-afec-4801-b5c7-27bed3f423aa"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHN8OOrDP9-M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea18029-fc27-4498-ac2a-f79af348f047"
      },
      "source": [
        "# Stance Detection\n",
        "import sys\n",
        "\n",
        "stance = drive.CreateFile({'id': 'https://drive.google.com/file/d/1LqUB3NMsp3ThF-PULsYRusiuPyHZ4Bt_/view?usp=sharing'.split('/')[-2]})\n",
        "stance.GetContentFile('AlternusVeraStanceDetection.py')\n",
        "\n",
        "#sys.path.append('/content/drive/MyDrive/MLFall2020/the-feature-finders/AlternusVera/Stance')\n",
        "import AlternusVeraStanceDetection as sp\n",
        "predictStance = sp.StanceDitectionFeature()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfkbqzPHQB-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb880be0-9a76-4044-8c2c-f2983c25f28e"
      },
      "source": [
        "#Toxicity\n",
        "toxicity = drive.CreateFile({'id': 'https://drive.google.com/file/d/16D6wtyGx6W6ahxQu1lHcr_NGyfDeMOzc/view?usp=sharing'.split('/')[-2]})\n",
        "toxicity.GetContentFile('alternusveratoxicity.py')\n",
        "\n",
        "#sys.path.append('/content/drive/MyDrive/MLFall2020/the-feature-finders/AlternusVera/Toxicity')\n",
        "import alternusveratoxicity as tx\n",
        "predictToxicity = tx.ToxicityFeature()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIrbd9nYR4bb"
      },
      "source": [
        "#Reliable Source\n",
        "reliable = drive.CreateFile({'id': 'https://drive.google.com/file/d/1gBhA7OtsZvR-pz829I9w_3zyG85QKM8M/view?usp=sharing'.split('/')[-2]})\n",
        "reliable.GetContentFile('AlternusVeraReliableSource.py')\n",
        "\n",
        "#sys.path.append('/content/drive/MyDrive/MLFall2020/the-feature-finders/AlternusVera/ReliableSource')\n",
        "import AlternusVeraReliableSource as rs\n",
        "reliablesource = rs.ReliableSource()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR0vzKDFQFhB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9de661f1-50b1-42a6-ea38-f57a82a86750"
      },
      "source": [
        "#TitlevsBody\n",
        "LABELS = ['agree', 'disagree', 'discuss', 'unrelated']\n",
        "#sys.path.append('/content/drive/MyDrive/MLFall2020/the-feature-finders/AlternusVera/TitleVsBody')\n",
        "\n",
        "titlebody = drive.CreateFile({'id': 'https://drive.google.com/file/d/1zp5xpiObWCL2_3DeV3xaCvKWAWxbvHCZ/view?usp=sharing'.split('/')[-2]})\n",
        "titlebody.GetContentFile('team_features_finders_factor_title_vs_body.py')\n",
        "\n",
        "import team_features_finders_factor_title_vs_body as titlevsbody\n",
        "tvb = titlevsbody.TitleVsBody()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgIzZI9MQNTs"
      },
      "source": [
        "# LABELS = ['agree', 'disagree', 'discuss', 'unrelated']\n",
        "def get_titleVsBodyScore(val): # input int, return between 0 and 1, being 0 = Fake,  1 = True\n",
        "    map = {'0': 1., '1': .2, '2': .8, '3': 0.}\n",
        "\n",
        "    return map[str(val)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJRs4ChPZQ-6"
      },
      "source": [
        "# Integrating Cereal Killers\n",
        "\n",
        "* Ramya Kandasamy - Sentiment Classification (BERT)\n",
        "* Jed Villanueva - Social Credibility\n",
        "* Raymond Ho - Psychological Utility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXwzGjsNZWZ9",
        "outputId": "d197e8e7-619a-46df-acaa-df64ccf27ee3"
      },
      "source": [
        "! git clone https://github.com/jedvillan/CerealKillers_AlternusVera.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'CerealKillers_AlternusVera'...\n",
            "remote: Enumerating objects: 124, done.\u001b[K\n",
            "remote: Counting objects: 100% (124/124), done.\u001b[K\n",
            "remote: Compressing objects: 100% (105/105), done.\u001b[K\n",
            "remote: Total 124 (delta 34), reused 25 (delta 4), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (124/124), 70.77 MiB | 8.52 MiB/s, done.\n",
            "Resolving deltas: 100% (34/34), done.\n",
            "Checking out files: 100% (38/38), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZ4bvUU_ZW8T",
        "outputId": "708228fc-af76-4fea-dea4-2f06a71a1f9b"
      },
      "source": [
        "! apt-get install git-lfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  git-lfs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 14 not upgraded.\n",
            "Need to get 2,129 kB of archives.\n",
            "After this operation, 7,662 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 git-lfs amd64 2.3.4-1 [2,129 kB]\n",
            "Fetched 2,129 kB in 2s (1,362 kB/s)\n",
            "Selecting previously unselected package git-lfs.\n",
            "(Reading database ... 144793 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_2.3.4-1_amd64.deb ...\n",
            "Unpacking git-lfs (2.3.4-1) ...\n",
            "Setting up git-lfs (2.3.4-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdpPEl4pZXWg",
        "outputId": "31a78859-7267-4651-f1b4-cc718c356559"
      },
      "source": [
        "% cd CerealKillers_AlternusVera\n",
        "! git fetch\n",
        "! git pull\n",
        "! git lfs pull\n",
        "% cd /content/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/CerealKillers_AlternusVera\n",
            "Already up to date.\n",
            "Git LFS: (1 of 1 files) 413.25 MB / 413.25 MB\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqlJDPR5ZWzg",
        "outputId": "39ca78de-8069-46fa-a16d-6f81276cda65"
      },
      "source": [
        "! pip install torch\n",
        "! pip install transformers==3.3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
            "Collecting transformers==3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/fc/18e56e5b1093052bacf6750442410423f3d9785d14ce4f54ab2ac6b112a6/transformers-3.3.0-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 9.1MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 24.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.3) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.3) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 42.9MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 55.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.3) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.3) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.3) (0.8)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.3) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.3) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.3) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.3) (0.17.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3) (2020.11.8)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=da41246def262443e3947a1f9f179051f7b76c50c8b162b22c0538f1c3b6c494\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.8.1rc2 transformers-3.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBwI792aZWRJ",
        "outputId": "303e7e62-55d0-49c4-81a9-f1edc1ef8476"
      },
      "source": [
        "! pip install autogluon # May need to restart runtime after installing fresh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: autogluon in /usr/local/lib/python3.6/dist-packages (0.0.14)\n",
            "Requirement already satisfied: openml in /usr/local/lib/python3.6/dist-packages (from autogluon) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from autogluon) (1.18.5)\n",
            "Requirement already satisfied: lightgbm<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from autogluon) (3.1.0)\n",
            "Requirement already satisfied: pyarrow<=1.0.0 in /usr/local/lib/python3.6/dist-packages (from autogluon) (0.14.1)\n",
            "Requirement already satisfied: catboost<0.25,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from autogluon) (0.24.3)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from autogluon) (0.8.4)\n",
            "Requirement already satisfied: gluoncv<1.0,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from autogluon) (0.8.0)\n",
            "Requirement already satisfied: tqdm>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from autogluon) (4.41.1)\n",
            "Requirement already satisfied: tornado>=5.0.1 in /usr/local/lib/python3.6/dist-packages (from autogluon) (5.1.1)\n",
            "Requirement already satisfied: cryptography>=2.8 in /usr/local/lib/python3.6/dist-packages (from autogluon) (3.2.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from autogluon) (0.29.21)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from autogluon) (2.23.0)\n",
            "Requirement already satisfied: autogluon-contrib-nlp in /usr/local/lib/python3.6/dist-packages (from autogluon) (0.0.1b20201009)\n",
            "Requirement already satisfied: ConfigSpace<=0.4.10 in /usr/local/lib/python3.6/dist-packages (from autogluon) (0.4.10)\n",
            "Requirement already satisfied: dask>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from autogluon) (2.12.0)\n",
            "Requirement already satisfied: fastparquet==0.4.1 in /usr/local/lib/python3.6/dist-packages (from autogluon) (0.4.1)\n",
            "Requirement already satisfied: scikit-learn<0.24,>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from autogluon) (0.22.2.post1)\n",
            "Requirement already satisfied: psutil<=5.7.0,>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from autogluon) (5.4.8)\n",
            "Requirement already satisfied: distributed>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from autogluon) (2.30.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from autogluon) (1.16.28)\n",
            "Requirement already satisfied: scipy>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from autogluon) (1.4.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from autogluon) (3.6.4)\n",
            "Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.6/dist-packages (from autogluon) (2.7.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from autogluon) (3.2.2)\n",
            "Requirement already satisfied: Pillow<=6.2.1 in /usr/local/lib/python3.6/dist-packages (from autogluon) (6.2.1)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.6/dist-packages (from autogluon) (0.8.1)\n",
            "Requirement already satisfied: pandas<2.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from autogluon) (1.1.4)\n",
            "Requirement already satisfied: networkx<3.0,>=2.3 in /usr/local/lib/python3.6/dist-packages (from autogluon) (2.5)\n",
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.6/dist-packages (from openml->autogluon) (0.12.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from openml->autogluon) (2.8.1)\n",
            "Requirement already satisfied: liac-arff>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from openml->autogluon) (2.5.0)\n",
            "Requirement already satisfied: six>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pyarrow<=1.0.0->autogluon) (1.15.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost<0.25,>=0.23.0->autogluon) (4.4.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from gluoncv<1.0,>=0.5.0->autogluon) (2.0.0)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.8->autogluon) (1.14.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->autogluon) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->autogluon) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->autogluon) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->autogluon) (2.10)\n",
            "Requirement already satisfied: contextvars in /usr/local/lib/python3.6/dist-packages (from autogluon-contrib-nlp->autogluon) (2.4)\n",
            "Requirement already satisfied: tokenizers<0.9.0,>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from autogluon-contrib-nlp->autogluon) (0.8.1rc2)\n",
            "Requirement already satisfied: flake8 in /usr/local/lib/python3.6/dist-packages (from autogluon-contrib-nlp->autogluon) (3.8.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from autogluon-contrib-nlp->autogluon) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses>=0.0.38 in /usr/local/lib/python3.6/dist-packages (from autogluon-contrib-nlp->autogluon) (0.0.43)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from autogluon-contrib-nlp->autogluon) (0.1.94)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from autogluon-contrib-nlp->autogluon) (0.1.8)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.6/dist-packages (from autogluon-contrib-nlp->autogluon) (1.4.14)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from autogluon-contrib-nlp->autogluon) (3.12.4)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<=0.4.10->autogluon) (2.4.7)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<=0.4.10->autogluon) (3.7.4.3)\n",
            "Requirement already satisfied: thrift>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from fastparquet==0.4.1->autogluon) (0.13.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastparquet==0.4.1->autogluon) (20.4)\n",
            "Requirement already satisfied: numba>=0.28 in /usr/local/lib/python3.6/dist-packages (from fastparquet==0.4.1->autogluon) (0.48.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn<0.24,>=0.22.0->autogluon) (0.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from distributed>=2.6.0->autogluon) (50.3.2)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.6.0->autogluon) (0.11.1)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.6.0->autogluon) (2.0.0)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.6.0->autogluon) (2.3.0)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.6.0->autogluon) (7.1.2)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.6.0->autogluon) (1.7.0)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.6.0->autogluon) (1.6.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from distributed>=2.6.0->autogluon) (3.13)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.6.0->autogluon) (1.0.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->autogluon) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->autogluon) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.20.0,>=1.19.28 in /usr/local/lib/python3.6/dist-packages (from boto3->autogluon) (1.19.28)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->autogluon) (1.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->autogluon) (1.9.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->autogluon) (20.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->autogluon) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->autogluon) (8.6.0)\n",
            "Requirement already satisfied: bcrypt>=3.1.3 in /usr/local/lib/python3.6/dist-packages (from paramiko>=2.4->autogluon) (3.2.0)\n",
            "Requirement already satisfied: pynacl>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from paramiko>=2.4->autogluon) (1.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->autogluon) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->autogluon) (1.3.1)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize->autogluon) (20.4.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas<2.0,>=1.0.0->autogluon) (2018.9)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx<3.0,>=2.3->autogluon) (4.4.2)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost<0.25,>=0.23.0->autogluon) (1.3.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.8->autogluon) (2.20)\n",
            "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars->autogluon-contrib-nlp->autogluon) (0.14)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from flake8->autogluon-contrib-nlp->autogluon) (2.0.0)\n",
            "Requirement already satisfied: pyflakes<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from flake8->autogluon-contrib-nlp->autogluon) (2.2.0)\n",
            "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from flake8->autogluon-contrib-nlp->autogluon) (0.6.1)\n",
            "Requirement already satisfied: pycodestyle<2.7.0,>=2.6.0a1 in /usr/local/lib/python3.6/dist-packages (from flake8->autogluon-contrib-nlp->autogluon) (2.6.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.28->fastparquet==0.4.1->autogluon) (0.31.0)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.6/dist-packages (from zict>=0.1.3->distributed>=2.6.0->autogluon) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->flake8->autogluon-contrib-nlp->autogluon) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezyAVZEsZvK7",
        "outputId": "9535fdb3-87d0-4079-90ed-8d8251ca51bd"
      },
      "source": [
        "! pip install --upgrade mxnet-cu100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mxnet-cu100\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/49/2876c87397592fdb2cca87928d538c9969adb7d271927ef36cb69d62fc63/mxnet_cu100-1.7.0-py2.py3-none-manylinux2014_x86_64.whl (827.8MB)\n",
            "\u001b[K     |████████████████████████████████| 827.8MB 18kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (0.8.4)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu100) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu100) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu100) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu100) (2020.11.8)\n",
            "Installing collected packages: mxnet-cu100\n",
            "Successfully installed mxnet-cu100-1.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvJwSIZ1Zv11"
      },
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append('/content/CerealKillers_AlternusVera/BERT')\n",
        "sys.path.append('/content/CerealKillers_AlternusVera/SC')\n",
        "sys.path.append('/content/CerealKillers_AlternusVera/PU')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBfiPtHpZviD",
        "outputId": "07d9ce0a-07ac-49e0-a02c-1c4803a60cd3"
      },
      "source": [
        "import socialcredibility as SC \n",
        "import psychologicalutility as PU\n",
        "import bert as BT\n",
        "\n",
        "sc = SC.CerealKillers_SocialCredibility()\n",
        "pu = PU.CerealKillers_PsychologicalUtility()\n",
        "bt = BT.CerealKillers_SentimentClassifier()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NumPy-shape semantics has been activated in your code. This is required for creating and manipulating scalar and zero-size tensors, which were not supported in MXNet before, as in the official NumPy library. Please DO NOT manually deactivate this semantics while using `mxnet.numpy` and `mxnet.numpy_extension` modules.\n",
            "100%|██████████| 472/472 [00:00<00:00, 229kiB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading /root/.mxnet/models/nlp/google_electra_small/model-9ffb21c8.yml from https://gluonnlp-numpy-data.s3-accelerate.amazonaws.com/models/google_electra_small/model-9ffb21c8.yml...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 323k/323k [00:00<00:00, 11.9MiB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading /root/.mxnet/models/nlp/google_electra_small/vocab-e6d2b21d.json from https://gluonnlp-numpy-data.s3-accelerate.amazonaws.com/models/google_electra_small/vocab-e6d2b21d.json...\n",
            "Downloading /root/.mxnet/models/nlp/google_electra_small/model-2654c8b4.params from https://gluonnlp-numpy-data.s3-accelerate.amazonaws.com/models/google_electra_small/model-2654c8b4.params...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 53.9M/53.9M [00:01<00:00, 52.1MiB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cn5YboUiZu6M",
        "outputId": "0b16c554-d7a1-432f-862a-64ea6cbb00d4"
      },
      "source": [
        "sc.search_user_by_name('barack-obama')\n",
        "sc_score = sc.predict(sc.get_user_data())\n",
        "print(sc_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R11pPZYtZ5VZ",
        "outputId": "6263ef58-7763-43cc-fd9b-424586b2a74f"
      },
      "source": [
        "pu_score = pu.predict(\"When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration.\",1,2,5,100,2)\n",
        "print(pu_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4316544632116953\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrlsB8yNZ5G4",
        "outputId": "f7a54ff3-0c73-42b3-f145-362d54921e3a"
      },
      "source": [
        "bt_score = bt.predict(\"When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration.\")\n",
        "print(bt_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1773: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30UA1sxaft0N"
      },
      "source": [
        "# The Shining Unicorns\n",
        "\n",
        "\n",
        "1.   Darshan Mandlecha - Content Statistics\n",
        "2.   Shubham Katariya - Echo Chamber\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpsj7HnwgE5B"
      },
      "source": [
        "shining_Unicorns_factors = drive.CreateFile({'id': 'https://drive.google.com/file/d/1cIALYdXYRZ0sZEvU9s3njxI6ggTH_HSU/view?usp=sharing'.split('/')[-2]})\n",
        "shining_Unicorns_factors.GetContentFile('finalteamintegration.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pI6GyJogYto",
        "outputId": "82a191ef-5e92-4aa4-e73f-f971eb710adc"
      },
      "source": [
        "import finalteamintegration as EM"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vaH3PFDsYzB"
      },
      "source": [
        "echoChamberMaster1=EM.EchoChamberMaster()\n",
        "contentStatistics1=EM.ContentStatistics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgKZfphP5Hsv"
      },
      "source": [
        "# Polynomial equation with normalised weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTPmSfb1YCZ_"
      },
      "source": [
        "def getProbablityScore(category):\n",
        "\n",
        "    if(category =='pants-fire' ):\n",
        "        return 0.9;\n",
        "    elif(category =='false'):\n",
        "        return 0.73;\n",
        "    elif(category =='full-flop' or category == 'barely-true'):\n",
        "        return 0.56;\n",
        "    elif(category =='half-true' or category == 'half-flip'):\n",
        "        return 0.4;\n",
        "    elif(category =='mostly-true'):\n",
        "        return 0.24;\n",
        "    else:\n",
        "        return 0.1;\n",
        "\n",
        "\n",
        "def isFakeNews(headline_text, body, partyaffiliation,source,context,mostly_true_count,half_true_count,barely_true_count,false_count,pants_on_fire_count):\n",
        "    headline_text = headline_text\n",
        "    #label_encode = label_encode\n",
        "    accur = [0.57, 0.56, 0.58, 0.56,\n",
        "             0.96, 0.64, 0.76, 0.47, \n",
        "             0.64, 0.56, 0.84, \n",
        "             0.87, 0.70, 0.82, 0.85,\n",
        "             0.78, 0.82, 0.48, 0.72,\n",
        "             0.75, 0.77, 0.76, 0.70,\n",
        "             0.26, 0.28, 0.49,\n",
        "             0.82, 0.35, 0.55,\n",
        "             0.58, 0.54] # using the (normalized) accuracy as weigths\n",
        "    w = [float(i)/sum(accur) for i in accur]\n",
        "    sumW = 0\n",
        "    prob = []\n",
        "    print(w)\n",
        "\n",
        "    print(\"here\")\n",
        "    #For Toxicity - GWC\n",
        "    if headline_text != \"\" :\n",
        "        score1 = GirlsWhoCode_Toxicity.getToxicityScore(headline_text)\n",
        "        print(\"probability of news not being fake from the toxicity model = \",score1)\n",
        "        prob.append(w[0] * score1)\n",
        "        sumW += w[0]\n",
        "\n",
        "    #For Topic Modelling for LDA - GWC\n",
        "    if headline_text != \"\" :\n",
        "        score2 = Topics_with_LDA_Bigram.getTopicScoreBigramLDAModel(headline_text)\n",
        "        prob.append(w[1] * score2)\n",
        "        print(\"probability of news not being fake from the topic model = \",score2)\n",
        "        sumW += w[1] \n",
        "\n",
        "    #For Bias - GWC\n",
        "    if headline_text != \"\":\n",
        "        score3 = Gwc_Bias.get_bias_score(headline_text)\n",
        "        print(\"probability of news not being fake from the bias model = \", score3)\n",
        "        prob.append(w[2] * score3)\n",
        "        sumW += w[2]\n",
        "\n",
        "    #For political_affiliaiton - GWC\n",
        "    if headline_text != \"\" and partyaffiliation != \"\":\n",
        "        score4 = Girlswhocode_PoliticalAfiiliation.DATAMINERS_getPartyAffiliationScore(headline_text,partyaffiliation)\n",
        "        print(\"probability of news not being fake political affilaiiton model = \", score4)\n",
        "        prob.append(w[3] * score4)\n",
        "        sumW += w[3]\n",
        "\n",
        "    #For Clickbait - Seekers\n",
        "    if ( headline_text!=\"\"):\n",
        "        prob.append(w[4] * Seekers_ClickBait().predict(headline_text))\n",
        "        sumW += w[4]\n",
        "    \n",
        "    #For Stance_detection - Seekers\n",
        "    if (headline_text != \"\"):\n",
        "\n",
        "        prob.append(w[5] * Seekers_StanceDetection().predict(headline_text))\n",
        "        sumW += w[5]\n",
        "    #For Spam - Seekers\n",
        "    if ( headline_text!=\"\"):\n",
        "\n",
        "        prob.append(w[6] * Seekers_Spam().predict(headline_text))\n",
        "        sumW += w[6]\n",
        "    #For Bert - Seekers\n",
        "    if (headline_text!=\"\"):\n",
        "\n",
        "        prediction, probability = Seekers_BertMcc.get_bert_predictions(headline_text,source,context,mostly_true_count,half_true_count,barely_true_count,false_count,pants_on_fire_count)\n",
        "        prob.append(w[7] * probability)\n",
        "        sumW += w[7]\n",
        "    #For Education - Trailblazers\n",
        "    if (headline_text != \"\"):\n",
        "        edu_mod_res = AVE.predictLable(headline_text)\n",
        "        edu_score=getProbablityScore(edu_mod_res)\n",
        "        print(\"Probability of news baded on education model is \",edu_score)\n",
        "        prob.append(w[8]* edu_score)\n",
        "        sumW+=w[8]\n",
        "\n",
        "    #For Event Coverage - Trailblazers\n",
        "        event_score = EC.predictLable(headline_text)\n",
        "        print(\"Probability of news baded on Event coverage model is \",event_score[0])\n",
        "        prob.append(w[9]* event_score[0])\n",
        "        sumW+=w[9]\n",
        "\n",
        "    #For Misleding intentions - Trailblazers\n",
        "        mislead_intent = (MI.predictIntention(headline_text))\n",
        "        mislead_score=getProbablityScore(mislead_intent)\n",
        "        print(\"Probability of news baded on misleading intension model is \",mislead_score)\n",
        "        prob.append(w[10]* mislead_score)\n",
        "        sumW+=w[10]\n",
        "    \n",
        "    # Stance Detection - Go ML\n",
        "    if body:\n",
        "        prob.append(w[11] * getStanceDetectionScore(body))\n",
        "        sumW += w[11]\n",
        "    # Clickbait - Go ML\n",
        "    if body:\n",
        "        prob.append(w[12] * getClickbaitScore(body))\n",
        "        sumW += w[12]\n",
        "    # News Coverage - Go ML\n",
        "    if body:\n",
        "        prob.append(w[13] * getNewsCoverageScore(body))\n",
        "        sumW += w[13]\n",
        "    # Writing Style - Go ML\n",
        "    if body:\n",
        "        prob.append(w[14] * WritingStyleScore(body))\n",
        "        sumW += w[14]\n",
        "\n",
        "    if (headline_text != \"\"):\n",
        "    # malicious account - Sigma\n",
        "        factorMA, _, _  = maObject.predict(malicious_accountCls, body, nlp)\n",
        "        prob.append(w[15] * factorMA ) \n",
        "        sumW += w[15]\n",
        "        \n",
        "      # credebility - Sigma\n",
        "        _ , factorCR = credObject.predict(credCls, body , nlp)\n",
        "        prob.append(w[16] * factorCR ) \n",
        "        sumW += w[16]\n",
        "\n",
        "      # Network base - Sigma\n",
        "        factorNB= nb.predict(body, nlp)\n",
        "        prob.append(w[17] * factorNB ) \n",
        "        sumW += w[17]\n",
        "\n",
        "      # Authenticity - Sigma\n",
        "        venue = 'CNN'\n",
        "        factorVA = va.predict(body, venue)\n",
        "        prob.append(w[18] * factorVA ) \n",
        "        sumW += w[18]\n",
        "\n",
        "      # Stance Detection\n",
        "        prob.append(w[19] * predictStance.FeatureFinders_getStanceScore(headline_text,body))\n",
        "        sumW += w[19]\n",
        "\n",
        "      # Reliable Source\n",
        "        prob.append(w[20] * reliablesource.FeatureFinders_getReliabilityBySource(source))\n",
        "        sumW += w[20]\n",
        "\n",
        "      # Toxicity\n",
        "        prob.append(w[21] * predictToxicity.FeatureFinders_getToxicityScore(headline_text,body))\n",
        "        sumW += w[21]\n",
        "      \n",
        "      # Title vs Body\n",
        "        tvb_value = tvb.FeatureFinders_getTitleVsBodyRelationship(head_line=headline_text, body_text=body)[0]\n",
        "        prob.append(w[22] * tvb.FeatureFinders_getTitleVsBodyScore(tvb_value)[0][1]) # 0 is Fake, 1 is True\n",
        "        sumW += w[22]\n",
        "\n",
        "    # Sentiment Classification\n",
        "    if headline_text != \"\":\n",
        "        bt_score = bt.predict(headline_text)\n",
        "        prob.append(w[26]* bt_score)\n",
        "        sumW+=w[26]\n",
        "\n",
        "    # # Psychological Utility\n",
        "    if body != \"\":\n",
        "        pu_score = pu.predict(body,barely_true_count,false_count,half_true_count,mostly_true_count,pants_on_fire_count)\n",
        "        prob.append(w[27]* pu_score)\n",
        "        sumW+=w[27]\n",
        "      \n",
        "    # Social Credibility\n",
        "    if source != \"\":\n",
        "        sc.search_user_by_name(source)\n",
        "        sc_score = sc.predict(sc.get_user_data())\n",
        "        prob.append(w[28]* sc_score)\n",
        "        sumW+=w[28]\n",
        "\n",
        "    # Content Statistics\n",
        "    if (headline_text != \"\"):\n",
        "        in_df = pd.DataFrame(data=[headline_text], columns=['Statement'])\n",
        "        res_cs = bcs.predict(in_df).replace('pants-fire', 1.0).replace('false', 0.8).replace('barely-true', 0.6).replace('half-true', 0.4).replace('mostly-true', 0.2).replace('true', 0.0)\n",
        "        prob.append(w[23] * float(res_cs.iloc[0]))\n",
        "        sumW+=w[23]\n",
        "\n",
        "    # Context Veracity\n",
        "    if (headline_text != \"\"):\n",
        "        prob.append(w[24]* float(cv.get_veracity_scores(headline_text)))\n",
        "        sumW+=w[24]\n",
        "\n",
        "    # Sensationalism\n",
        "    if (headline_text != \"\"):\n",
        "        prob.append(w[25] * 0.75 * sensa.getScore(headline_text))\n",
        "        sumW+=w[25]\n",
        "\n",
        "    #For Content Statistics\n",
        "    if (headline_text != \"\"):\n",
        "        content_Statistics_score = contentStatistics1.predict(headline_text)\n",
        "        print(\"Probability of news baded on content Statistics model is \",content_Statistics_score)\n",
        "        prob.append(w[29]* content_Statistics_score)\n",
        "        sumW+=w[29]\n",
        "    #For Echo Chamber\n",
        "    if (headline_text != \"\"):\n",
        "        echo_ChamberMaster_score = echoChamberMaster1.predict(headline_text)\n",
        "        print(\"Probability of news baded on echo Chamber model is \",echo_ChamberMaster_score)\n",
        "        prob.append(w[30]* echo_ChamberMaster_score)\n",
        "        sumW+=w[30]\n",
        "\n",
        "    \n",
        "   \n",
        "    probTotal = sum(prob[0:len(prob)]) / sumW\n",
        "    return probTotal\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6_MHiScDN8x"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8o1FciVZwau",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bffbd02a-71f7-4e32-b453-ffe12167ff25"
      },
      "source": [
        "result = isFakeNews('Media fawns over Biden\\'s Cabinet rollout, describes being rescued from this craziness by superheroes','Members of the mainstream media showered President-elect Joe Biden with praise Tuesday following the rollout of his foreign policy and national security team. Biden formally introduced several Cabinet nominees and White House advisers, including Secretary of State nominee Antony Blinken, National Security Adviser pick Jake Sullivan, U.S. Ambassador to the United Nations choice Linda Thomas-Greenfield, and newly minted Special Presidential Envoy for Climate John Kerry. MEDIA OBSESSES OVER ANTONY BLINKEN\\'S GUITAR SKILLS AFTER BIDEN ANNOUNCES HIM AS SECRETARY OF STATE NOMINEEDuring an MSNBC panel discussion, PBS NewsHour White House correspondent Yamiche Alcindor appeared to agree with what she said a Democrat source had told her. I was talking to a Democrat who just said this also felt like \\'The Avengers, Alcindor said. It felt like we were being rescued from this craziness that we\\'ve all lived through from the last four years and now here are the superheroes to come and save us all.', 'democrat', 'fox news','in a report', 0,50,50,0,0)\n",
        "\n",
        "if(result>=0.8):\n",
        "    print(\"pants-on-fire\")\n",
        "elif(result<0.80 and result>=0.64):\n",
        "    print(\"false\")\n",
        "elif(result <0.64 and result >=0.48):\n",
        "    print(\"barely-true\")\n",
        "elif(result<0.48 and result>=0.32):\n",
        "    print(\"half-true\")\n",
        "elif(result<0.32 and result>=(0.16)):\n",
        "    print('mostly-true')\n",
        "else:\n",
        "    print('true')\n",
        "\n",
        "#if result >= 0.5:\n",
        "#     print(\"is FAKE NEWS!!!\")\n",
        "# else:\n",
        "#     print(\"it is NOT fake news!!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.03315881326352531, 0.032577079697498554, 0.03374054682955207, 0.032577079697498554, 0.05584642233856894, 0.03723094822571263, 0.04421175101803375, 0.02734147760325771, 0.03723094822571263, 0.032577079697498554, 0.048865619546247824, 0.05061082024432811, 0.04072134962187318, 0.0477021524141943, 0.049447353112274585, 0.04537521815008727, 0.0477021524141943, 0.02792321116928447, 0.041884816753926704, 0.04363001745200699, 0.04479348458406051, 0.04421175101803375, 0.04072134962187318, 0.015125072716695756, 0.016288539848749277, 0.02850494473531123]\n",
            "here\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "                                       headline_text\n",
            "0  Media fawns over Biden's Cabinet rollout, desc...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "                                       headline_text toxicity\n",
            "0  medium fawn biden cabinet rollout describ resc...        2\n",
            "                                       headline_text  ... sentiment_encode\n",
            "0  medium fawn biden cabinet rollout describ resc...  ...         2.098612\n",
            "\n",
            "[1 rows x 4 columns]\n",
            "probability of news not being fake from the toxicity model =  0.44903368636243024\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[['media_fawns', 'fawns_bidens', 'bidens_cabinet', 'cabinet_rollout', 'rollout_describes', 'describes_rescued', 'rescued_craziness', 'craziness_superheroes']]\n",
            "[{'sentiment_score': {'neg': 0.149, 'neu': 0.69, 'pos': 0.161, 'compound': 0.0516}, 'sentiment_label': 'neutral', 'sentiment_label_encode': 2.09861228866811}]\n",
            "                                                News  ... Topic_Score\n",
            "0  Media fawns over Biden's Cabinet rollout, desc...  ...      0.9401\n",
            "\n",
            "[1 rows x 6 columns]\n",
            "probability of news not being fake from the topic model =  0.4305878508964852\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "probability of news not being fake from the bias model =  0.9996785585546136\n",
            "Index(['headline_text', 'partyaffiliation'], dtype='object')\n",
            "Index(['headline_text', 'partyaffiliation', 'partyaffiliation_encode',\n",
            "       'republican_vector', 'democrats_vector', 'liberterian_vector'],\n",
            "      dtype='object')\n",
            "Index(['headline_text', 'partyaffiliation', 'partyaffiliation_encode',\n",
            "       'republican_vector', 'democrats_vector', 'liberterian_vector', 'tfidf',\n",
            "       'issues_score', 'sentiment', 'sentiment_encode'],\n",
            "      dtype='object')\n",
            "Index(['headline_text', 'subject', 'speaker', 'speakerjobtitle', 'stateinfo',\n",
            "       'partyaffiliation', 'context', 'encoded_label'],\n",
            "      dtype='object')\n",
            "here in LDA\n",
            "                                       headline_text  ... topic_score\n",
            "0  media fawn biden cabinet rollout describ rescu...  ...    0.523906\n",
            "\n",
            "[1 rows x 12 columns]\n",
            "tagged text [TaggedDocument(words=['media', 'fawn', 'biden', 'cabinet', 'rollout', 'describ', 'rescu', 'crazi', 'superhero'], tags=[2])]\n",
            "before\n",
            "after\n",
            "probability of news not being fake political affilaiiton model =  0.43560109278719694\n",
            "/content\n",
            "John McCain\n",
            "Probability of news baded on education model is  0.73\n",
            "Probability of news baded on Event coverage model is  0.39483999772424655\n",
            "Probability of news baded on misleading intension model is  0.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:00, 214.95it/s]\n",
            "1it [00:00, 1583.35it/s]\n",
            "1it [00:00, 461.57it/s]\n",
            "1it [00:00, 429.74it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0]\n",
            "\n",
            "predicted titleVsBody: [2]\n",
            "barely-true\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsBh7cmDy7h7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa471e09-4a1f-4665-8032-c27305a6c429"
      },
      "source": [
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.54441428]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}